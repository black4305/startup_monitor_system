#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ Apple Silicon í˜¸í™˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë”
ì½”ë©ì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œì»¬ Apple Silicon Macì—ì„œ ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import logging
import pickle
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import json
import re
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from typing import Dict, Any, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ë””ë°”ì´ìŠ¤ ì„¤ì • (Apple Silicon ìš°ì„ )
if torch.backends.mps.is_available():
    device = torch.device("mps")
    device_name = "Apple Silicon GPU (MPS)"
elif torch.cuda.is_available():
    device = torch.device("cuda")
    device_name = "NVIDIA GPU (CUDA)"
else:
    device = torch.device("cpu")
    device_name = "CPU"

logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device_name}")

class AppleSiliconFeatureExtractor:
    """Apple Silicon í˜¸í™˜ íŠ¹ì„± ì¶”ì¶œê¸°"""
    
    def __init__(self, device='mps'):
        self.device = device
        logger.info(f"ğŸ”§ íŠ¹ì„± ì¶”ì¶œê¸° ë””ë°”ì´ìŠ¤: {device}")
        
        # TF-IDF ë²¡í„°ë¼ì´ì €
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=None,
            ngram_range=(1, 2)
        )
        
        # Sentence Transformer ëª¨ë¸
        try:
            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)
            logger.info("âœ… Sentence Transformer ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"Sentence Transformer ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.sentence_model = None
        
        # í‘œì¤€í™” ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler = StandardScaler()
        
        # í›ˆë ¨ ìƒíƒœ
        self.is_fitted = False
    
    def fit(self, texts: List[str]):
        """íŠ¹ì„± ì¶”ì¶œê¸° í›ˆë ¨"""
        logger.info("ğŸ”§ íŠ¹ì„± ì¶”ì¶œê¸° í›ˆë ¨ ì¤‘...")
        
        # TF-IDF í›ˆë ¨
        self.tfidf_vectorizer.fit(texts)
        logger.info("âœ… TF-IDF í›ˆë ¨ ì™„ë£Œ")
        
        self.is_fitted = True
    
    def transform(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜"""
        if not self.is_fitted:
            raise ValueError("íŠ¹ì„± ì¶”ì¶œê¸°ê°€ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit() ë©”ì„œë“œë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        logger.info("ğŸ” íŠ¹ì„± ì¶”ì¶œ ì‹œì‘...")
        features_list = []
        
        # 1. TF-IDF íŠ¹ì„±
        logger.info("ğŸ“Š TF-IDF íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        tfidf_features = self.tfidf_vectorizer.transform(texts).toarray()
        features_list.append(tfidf_features)
        
        # 2. Sentence Embedding íŠ¹ì„± (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if self.sentence_model is not None:
            try:
                logger.info("ğŸ§  Sentence Embedding íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
                
                # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
                batch_size = 32
                sentence_features_list = []
                
                for i in range(0, len(texts), batch_size):
                    batch_texts = texts[i:i+batch_size]
                    batch_embeddings = self.sentence_model.encode(
                        batch_texts,
                        convert_to_tensor=False,
                        show_progress_bar=False,
                        batch_size=min(batch_size, len(batch_texts))
                    )
                    sentence_features_list.extend(batch_embeddings)
                
                sentence_features = np.array(sentence_features_list)
                logger.info(f"âœ… Sentence Embedding í˜•íƒœ: {sentence_features.shape}")
                features_list.append(sentence_features)
                
            except Exception as e:
                logger.warning(f"Sentence Embedding ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 3. ìˆ˜ë™ íŠ¹ì„± ì¶”ì¶œ
        logger.info("ğŸ”§ ìˆ˜ë™ íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        manual_features = np.array([
            [len(text), len(text.split()), text.count('ì§€ì›'), text.count('ì‚¬ì—…'), 
             text.count('ê¸°ì—…'), text.count('ì°½ì—…'), len(text.split('.')), text.count('?')]
            for text in texts
        ])
        features_list.append(manual_features)
        
        # ëª¨ë“  íŠ¹ì„± ê²°í•©
        combined_features = np.hstack(features_list)
        logger.info(f"âœ… ì „ì²´ íŠ¹ì„± í˜•íƒœ: {combined_features.shape}")
        
        return combined_features

class DeepSupportClassifier(nn.Module):
    """ê³ ì„±ëŠ¥ ë‹¤ì¸µ ì‹ ê²½ë§ ë¶„ë¥˜ê¸° (8ì¸µ Deep Network)"""
    
    def __init__(self, input_dim, hidden_dims=[1024, 512, 256, 128, 64, 32], dropout_rate=0.3):
        super(DeepSupportClassifier, self).__init__()
        
        # 8ì¸µ ê¹Šì€ ì‹ ê²½ë§ êµ¬ì¡° ì„¤ê³„
        layers = []
        prev_dim = input_dim
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ (ì…ë ¥ì¸µ â†’ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ)
        layers.append(nn.Linear(prev_dim, hidden_dims[0]))
        layers.append(nn.BatchNorm1d(hidden_dims[0]))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout_rate))
        prev_dim = hidden_dims[0]
        
        # ì¤‘ê°„ ì€ë‹‰ì¸µë“¤ (2~7ì¸µ)
        for hidden_dim in hidden_dims[1:]:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        
        # ì¶œë ¥ì¸µ (8ë²ˆì§¸ ì¸µ)
        layers.append(nn.Linear(prev_dim, 2))  # ì´ì§„ ë¶„ë¥˜
        
        self.network = nn.Sequential(*layers)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Xavier Uniform)
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

class AppleSiliconModelLoader:
    """ì½”ë© í›ˆë ¨ ëª¨ë¸ì„ Apple Siliconì—ì„œ ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, device='cpu'):
        self.device = device
        self.model = None
        self.feature_extractor = None
        self.is_loaded = False
        
    def load_model(self, model_path='apple_silicon_production_model.pkl'):
        """ì½”ë©ì—ì„œ í›ˆë ¨í•œ ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ğŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        
        try:
            # ëª¨ë¸ ë°ì´í„° ë¡œë“œ (Apple Silicon í˜¸í™˜)
            # pickle ë‚´ë¶€ì˜ torch í…ì„œë„ CPUë¡œ ë§¤í•‘
            import torch
            original_load = torch.load
            torch.load = lambda *args, **kwargs: original_load(*args, **kwargs, map_location='cpu') if 'map_location' not in kwargs else original_load(*args, **kwargs)
            
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            # ì›ë˜ torch.load ë³µì›
            torch.load = original_load
            
            logger.info(f"âœ… ëª¨ë¸ íƒ€ì…: {model_data['model_type']}")
            logger.info(f"âœ… ìƒì„± ì‹œê°„: {model_data['created_at']}")
            logger.info(f"âœ… í›ˆë ¨ ì •í™•ë„: {model_data['training_accuracy']:.4f}")
            
            # íŠ¹ì„± ì¶”ì¶œê¸° ë³µì›
            self.feature_extractor = model_data['feature_extractor']
            logger.info("âœ… íŠ¹ì„± ì¶”ì¶œê¸° ë¡œë“œ ì™„ë£Œ")
            
            # ë”¥ëŸ¬ë‹ ëª¨ë¸ ë³µì›
            if 'model_state_dict' in model_data:
                # ëª¨ë¸ êµ¬ì¡° ì •ë³´ì—ì„œ input_dim ì¶”ì¶œ
                model_structure = model_data['model_structure']
                input_dim = model_structure.network[0].in_features
                
                # ëª¨ë¸ ì¬ìƒì„±
                self.model = DeepSupportClassifier(input_dim=input_dim)
                
                # Apple Silicon í˜¸í™˜ì„ ìœ„í•œ map_location ì„¤ì •
                state_dict = model_data['model_state_dict']
                if isinstance(state_dict, dict):
                    # CPUë¡œ ë§¤í•‘í•˜ì—¬ ë¡œë“œ
                    self.model.load_state_dict(state_dict)
                else:
                    # torch.loadê°€ í•„ìš”í•œ ê²½ìš°
                    import tempfile
                    with tempfile.NamedTemporaryFile(suffix='.pth', delete=False) as tmp:
                        torch.save(state_dict, tmp.name)
                        self.model.load_state_dict(torch.load(tmp.name, map_location='cpu'))
                
                self.model = self.model.to(self.device)
                self.model.eval()
                
                logger.info(f"âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì…ë ¥ ì°¨ì›: {input_dim})")
            else:
                raise ValueError("ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            self.is_loaded = True
            logger.info("ğŸ‰ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
    def predict(self, texts):
        """ì˜ˆì¸¡"""
        if not self.is_loaded:
            raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # íŠ¹ì„± ì¶”ì¶œ
        features = self.feature_extractor.transform(texts)
        features_tensor = torch.FloatTensor(features).to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(features_tensor)
            _, predicted = torch.max(outputs.data, 1)
            
        return predicted.cpu().numpy()
    
    def predict_proba(self, texts):
        """í™•ë¥  ì˜ˆì¸¡"""
        if not self.is_loaded:
            raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # íŠ¹ì„± ì¶”ì¶œ
        features = self.feature_extractor.transform(texts)
        features_tensor = torch.FloatTensor(features).to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(features_tensor)
            probabilities = F.softmax(outputs, dim=1)
            
        return probabilities.cpu().numpy()

def test_model():
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    logger.info("ï¿½ï¿½ Apple Silicon ë”¥ëŸ¬ë‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    
    # ëª¨ë¸ ë¡œë” ìƒì„±
    loader = AppleSiliconModelLoader(device=device)
    loader.load_model('../apple_silicon_production_model.pkl')
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "ì¤‘ì†Œê¸°ì—… ì°½ì—… ì§€ì› ì‚¬ì—…ì— ì°¸ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
        "ì˜¤ëŠ˜ ì¹œêµ¬ì™€ ë§›ìˆëŠ” ì €ë…ì„ ë¨¹ì—ˆìŠµë‹ˆë‹¤", 
        "ê¸°ìˆ ê°œë°œ R&D ì§€ì› í”„ë¡œê·¸ë¨ ì•ˆë‚´",
        "ì£¼ë§ì— ì˜í™”ë¥¼ ë³´ëŸ¬ ê°ˆ ì˜ˆì •ì…ë‹ˆë‹¤",
        "ë²¤ì²˜ê¸°ì—… íˆ¬ì ìœ ì¹˜ ì§€ì›ì‚¬ì—…",
        "ìƒˆë¡œìš´ ë“œë¼ë§ˆê°€ ì¬ë¯¸ìˆì—ˆìŠµë‹ˆë‹¤",
        "ìŠ¤íƒ€íŠ¸ì—… ìœ¡ì„± í”„ë¡œê·¸ë¨ ëª¨ì§‘",
        "í—¬ìŠ¤ì¥ì—ì„œ ìš´ë™ì„ í–ˆìŠµë‹ˆë‹¤"
    ]
    
    logger.info("\nğŸ” ì˜ˆì¸¡ ê²°ê³¼:")
    predictions = loader.predict(test_texts)
    probabilities = loader.predict_proba(test_texts)
    
    for i, text in enumerate(test_texts):
        pred_label = "ì§€ì›ì‚¬ì—…" if predictions[i] == 1 else "ì¼ë°˜í…ìŠ¤íŠ¸"
        confidence = probabilities[i][predictions[i]]
        logger.info(f"ğŸ“ '{text}'")
        logger.info(f"   â†’ {pred_label} (ì‹ ë¢°ë„: {confidence:.3f})")
        logger.info()

if __name__ == "__main__":
    logger.info("ğŸ Apple Silicon ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë” ì‹œì‘!")
    test_model() 