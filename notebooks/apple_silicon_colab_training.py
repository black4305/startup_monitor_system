# ============================================
# ì…€ 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ê°œë³„ ì„¤ì¹˜)
# ============================================

# ê° ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì„¤ì¹˜
# !pip install torch
# !pip install transformers
# !pip install sentence-transformers
# !pip install scikit-learn
# !pip install pandas
# !pip install numpy
# !pip install matplotlib
# !pip install seaborn
# !pip install requests
# !pip install beautifulsoup4
# !pip install tqdm
# !pip install joblib
# !pip install PyMuPDF

print("ðŸ“¦ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!")

# ============================================
# ì…€ 2: ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° Apple Silicon í˜¸í™˜ ë””ë°”ì´ìŠ¤ ì„¤ì •
# ============================================
import torch
import numpy as np
import pandas as pd
import joblib
import json
import re
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split
from sentence_transformers import SentenceTransformer
import nltk
from tqdm import tqdm
import pickle
import fitz  # PyMuPDF
import matplotlib.pyplot as plt
import seaborn as sns

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)

# Apple Silicon í˜¸í™˜ ë””ë°”ì´ìŠ¤ ì„¤ì •
def setup_device():
    """Apple Silicon (MPS) ë° CPU í˜¸í™˜ ë””ë°”ì´ìŠ¤ ì„¤ì •"""
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        device_name = "Apple Silicon (MPS)"
    elif torch.cuda.is_available():
        device = torch.device('cuda')
        device_name = "NVIDIA GPU (CUDA)"
    else:
        device = torch.device('cpu')
        device_name = "CPU"
    
    print(f"ðŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device_name}")
    return device

device = setup_device()

# Apple Silicon í˜¸í™˜ì„ ìœ„í•œ ì„¤ì •
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° ë””ë°”ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")

# ============================================
# ì…€ 3: PDFì—ì„œ ì§€ì›ì‚¬ì—… ë°ì´í„° ì¶”ì¶œ
# ============================================
def extract_support_programs_from_pdf(pdf_path):
    """PDFì—ì„œ ì§€ì›ì‚¬ì—… ì œëª©ë“¤ì„ ì¶”ì¶œí•˜ì—¬ JSON í˜•íƒœë¡œ ë³€í™˜"""
    
    # PDF íŒŒì¼ ì—…ë¡œë“œ (ì½”ëž©ì—ì„œëŠ” files.upload() ì‚¬ìš©)
    from google.colab import files
    print("ðŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”...")
    uploaded = files.upload()
    pdf_path = list(uploaded.keys())[0]
    
    support_programs = []
    
    try:
        # PDF ì—´ê¸°
        doc = fitz.open(pdf_path)
        print(f"ðŸ“– PDF íŽ˜ì´ì§€ ìˆ˜: {len(doc)} íŽ˜ì´ì§€")
        
        for page_num in tqdm(range(len(doc)), desc="PDF íŽ˜ì´ì§€ ì²˜ë¦¬"):
            page = doc.load_page(page_num)
            text = page.get_text()
            
            # ì§€ì›ì‚¬ì—… ì œëª© íŒ¨í„´ ì°¾ê¸° (ìˆ«ìž. ì œëª© í˜•íƒœ)
            patterns = [
                r'\d+\.\s*([ê°€-íž£\s\w\(\)]+(?:ì§€ì›|ì‚¬ì—…|ìœ¡ì„±|ê°œë°œ|ì°½ì—…|í˜ì‹ |ì—°êµ¬|ê¸°ìˆ |íˆ¬ìž|ìœµìž|ë³´ì¡°|í™œì„±í™”))',
                r'([ê°€-íž£\s\w\(\)]+(?:ì§€ì›ì‚¬ì—…|ì°½ì—…ì§€ì›|ê¸°ì—…ì§€ì›|R&D|ì—°êµ¬ê°œë°œ|ê¸°ìˆ ê°œë°œ|ì‚¬ì—…í™”))',
                r'ã€Œ([ê°€-íž£\s\w\(\)]+)ã€',  # ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ì‚¬ì—…ëª…
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, text, re.MULTILINE)
                for match in matches:
                    title = match.strip()
                    if len(title) > 5 and len(title) < 100:  # ì ì ˆí•œ ê¸¸ì´ í•„í„°ë§
                        support_programs.append({
                            'title': title,
                            'page': page_num + 1,
                            'category': 'support_program',
                            'label': 1
                        })
        
        doc.close()
        print(f"âœ… ì´ {len(support_programs)}ê°œ ì§€ì›ì‚¬ì—… ì¶”ì¶œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        support_programs = generate_sample_support_programs()
    
    return support_programs

def generate_sample_support_programs():
    """ìƒ˜í”Œ ì§€ì›ì‚¬ì—… ë°ì´í„° ìƒì„± (PDF íŒŒì‹± ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ)"""
    sample_programs = [
        "ì°½ì—…ê¸°ì—… ì§€ì›ì‚¬ì—…", "ì¤‘ì†Œê¸°ì—… ê¸°ìˆ ê°œë°œ ì§€ì›", "ì²­ë…„ì°½ì—… ìœ¡ì„±ì‚¬ì—…",
        "ìŠ¤íƒ€íŠ¸ì—… íˆ¬ìž ì—°ê³„ í”„ë¡œê·¸ëž¨", "ë²¤ì²˜ê¸°ì—… R&D ì§€ì›", "ì†Œìƒê³µì¸ ê²½ì˜ì•ˆì • ì§€ì›",
        "ê¸°ì—… ë””ì§€í„¸ ì „í™˜ ì§€ì›ì‚¬ì—…", "í˜ì‹ ê¸°ì—… ì„±ìž¥ ì§€ì›", "ì¤‘ì†Œê¸°ì—… ìˆ˜ì¶œ ì§€ì›",
        "ì°½ì—…ë³´ìœ¡ì„¼í„° ìš´ì˜ ì§€ì›", "ê¸°ìˆ ì‚¬ì—…í™” ì´‰ì§„ì‚¬ì—…", "ì¤‘ì†Œê¸°ì—… ìœµìž ì§€ì›",
        "ì²­ë…„ ì·¨ì—… ì§€ì› í”„ë¡œê·¸ëž¨", "ê¸°ì—… ì¸ë ¥ì–‘ì„± ì§€ì›", "ì¤‘ì†Œê¸°ì—… ì»¨ì„¤íŒ… ì§€ì›",
        "ìŠ¤ë§ˆíŠ¸ê³µìž¥ êµ¬ì¶• ì§€ì›", "ì¹œí™˜ê²½ ê¸°ì—… ì§€ì›ì‚¬ì—…", "ì§€ì—­ í˜ì‹  í´ëŸ¬ìŠ¤í„° ì¡°ì„±",
        "ì—¬ì„±ê¸°ì—… ì§€ì›ì‚¬ì—…", "ì‚¬íšŒì ê¸°ì—… ìœ¡ì„± ì§€ì›", "ë†ì´Œìœµë³µí•©ì‚°ì—… í™œì„±í™” ì§€ì›"
    ]
    
    programs = []
    for i, title in enumerate(sample_programs):
        programs.append({
            'title': title,
            'page': i // 3 + 1,
            'category': 'support_program', 
            'label': 1
        })
    
    return programs

# PDFì—ì„œ ì§€ì›ì‚¬ì—… ë°ì´í„° ì¶”ì¶œ
print("ðŸ“Š PDFì—ì„œ ì§€ì›ì‚¬ì—… ë°ì´í„° ì¶”ì¶œ ì¤‘...")
support_data = extract_support_programs_from_pdf("support_programs.pdf")

# ============================================
# ì…€ 4: ì¼ë°˜ í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
# ============================================
def generate_general_text_data(num_samples=500):
    """ì§€ì›ì‚¬ì—…ì´ ì•„ë‹Œ ì¼ë°˜ í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
    
    general_categories = {
        "ì¼ìƒìƒí™œ": ["ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ ì‚°ì±…ì„ í–ˆë‹¤", "ì¹œêµ¬ì™€ ì¹´íŽ˜ì—ì„œ ì»¤í”¼ë¥¼ ë§ˆì…¨ë‹¤", "ìƒˆë¡œìš´ ë“œë¼ë§ˆë¥¼ ì‹œì²­í–ˆë‹¤"],
        "ì·¨ë¯¸í™œë™": ["ì£¼ë§ì— ë“±ì‚°ì„ ë‹¤ë…€ì™”ë‹¤", "ìƒˆë¡œìš´ ìš”ë¦¬ ë ˆì‹œí”¼ë¥¼ ì‹œë„í•´ë´¤ë‹¤", "ë…ì„œ ëª¨ìž„ì— ì°¸ì—¬í–ˆë‹¤"],
        "í•™ìŠµêµìœ¡": ["ì˜¨ë¼ì¸ ê°•ì˜ë¥¼ ìˆ˜ê°•í•˜ê³  ìžˆë‹¤", "ìƒˆë¡œìš´ ì–¸ì–´ë¥¼ ë°°ìš°ê³  ìžˆë‹¤", "ìžê²©ì¦ ì‹œí—˜ì„ ì¤€ë¹„ ì¤‘ì´ë‹¤"],
        "ì—¬í–‰ê´€ê´‘": ["ì œì£¼ë„ ì—¬í–‰ì„ ê³„íší•˜ê³  ìžˆë‹¤", "í•´ì™¸ì—¬í–‰ ì¤€ë¹„ë¥¼ í•˜ê³  ìžˆë‹¤", "ë§›ì§‘ íƒë°©ì„ í–ˆë‹¤"],
        "ê±´ê°•ìš´ë™": ["í—¬ìŠ¤ìž¥ì—ì„œ ìš´ë™ì„ í–ˆë‹¤", "ìš”ê°€ ìˆ˜ì—…ì— ì°¸ì—¬í–ˆë‹¤", "ê±´ê°•í•œ ì‹ë‹¨ì„ ìœ ì§€í•˜ê³  ìžˆë‹¤"],
        "ë¬¸í™”ì˜ˆìˆ ": ["ë¯¸ìˆ ê´€ ì „ì‹œíšŒë¥¼ ê´€ëžŒí–ˆë‹¤", "ì½˜ì„œíŠ¸ì— ë‹¤ë…€ì™”ë‹¤", "ì˜í™” ê´€ëžŒì„ í–ˆë‹¤"],
        "ì‡¼í•‘ì†Œë¹„": ["ì˜¨ë¼ì¸ ì‡¼í•‘ì„ í–ˆë‹¤", "ìƒˆë¡œìš´ ì˜·ì„ êµ¬ë§¤í–ˆë‹¤", "í• ì¸ ìƒí’ˆì„ ì°¾ê³  ìžˆë‹¤"],
        "ê´€ê³„ì†Œí†µ": ["ê°€ì¡±ê³¼ ì‹œê°„ì„ ë³´ëƒˆë‹¤", "ë™ë£Œë“¤ê³¼ ì‹ì‚¬ë¥¼ í–ˆë‹¤", "ìƒˆë¡œìš´ ì‚¬ëžŒë“¤ì„ ë§Œë‚¬ë‹¤"]
    }
    
    general_data = []
    categories = list(general_categories.keys())
    
    for i in range(num_samples):
        category = np.random.choice(categories)
        base_texts = general_categories[category]
        text = np.random.choice(base_texts)
        
        general_data.append({
            'title': text,
            'category': 'general_text',
            'label': 0,
            'source': category
        })
    
    return general_data

# ì¼ë°˜ í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
print("ðŸ“ ì¼ë°˜ í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
general_data = generate_general_text_data(len(support_data))

# ============================================
# ì…€ 5: ë°ì´í„° ê²°í•© ë° ì „ì²˜ë¦¬
# ============================================
# ë°ì´í„° ê²°í•©
all_data = support_data + general_data
df = pd.DataFrame(all_data)

print(f"ðŸ“Š ì „ì²´ ë°ì´í„° ìˆ˜: {len(df)}")
print(f"ðŸ“ˆ ì§€ì›ì‚¬ì—… ë°ì´í„°: {len(support_data)}ê°œ")
print(f"ðŸ“ ì¼ë°˜ í…ìŠ¤íŠ¸ ë°ì´í„°: {len(general_data)}ê°œ")

# ë°ì´í„° ë¶„í¬ í™•ì¸
print("\nðŸ“Š ë ˆì´ë¸” ë¶„í¬:")
print(df['label'].value_counts())

# ìƒ˜í”Œ ë°ì´í„° í™•ì¸
print("\nðŸ“‹ ì§€ì›ì‚¬ì—… ìƒ˜í”Œ:")
print(df[df['label'] == 1]['title'].head(5).tolist())
print("\nðŸ“‹ ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
print(df[df['label'] == 0]['title'].head(5).tolist())

# ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ìž¥
training_data_json = df.to_dict('records')
with open('training_data.json', 'w', encoding='utf-8') as f:
    json.dump(training_data_json, f, ensure_ascii=False, indent=2)

print("âœ… í›ˆë ¨ ë°ì´í„° JSON íŒŒì¼ ì €ìž¥ ì™„ë£Œ")

# ============================================
# ì…€ 6: ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œê¸° ì •ì˜
# ============================================
class AppleSiliconFeatureExtractor:
    """Apple Silicon í˜¸í™˜ ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œê¸°"""
    
    def __init__(self, device='cpu'):
        self.device = device
        print(f"ðŸ”§ íŠ¹ì„± ì¶”ì¶œê¸° ë””ë°”ì´ìŠ¤: {device}")
        
        # TF-IDF ë²¡í„°ë¼ì´ì €
        self.tfidf = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            stop_words=None,
            lowercase=True
        )
        self.tfidf_fitted = False
        
        # Sentence Transformer (Apple Silicon í˜¸í™˜)
        try:
            self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            if str(device) != 'cpu':
                self.sentence_model = self.sentence_model.to(device)
            print("âœ… Sentence Transformer ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Sentence Transformer ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.sentence_model = None
        
        # ì§€ì›ì‚¬ì—… ê´€ë ¨ í‚¤ì›Œë“œ
        self.support_keywords = [
            'ì§€ì›', 'ì‚¬ì—…', 'ì°½ì—…', 'ê°œë°œ', 'ì—°êµ¬', 'R&D', 'ê¸°ìˆ ', 'í˜ì‹ ',
            'íˆ¬ìž', 'ìœµìž', 'ë³´ì¡°', 'ìœ¡ì„±', 'í™œì„±í™”', 'ì´‰ì§„', 'ê¸°ì—…',
            'ì¤‘ì†Œê¸°ì—…', 'ì†Œìƒê³µì¸', 'ë²¤ì²˜', 'ìŠ¤íƒ€íŠ¸ì—…', 'í´ëŸ¬ìŠ¤í„°'
        ]
        
    def extract_manual_features(self, texts):
        """ìˆ˜ë™ íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        for text in texts:
            text_lower = text.lower()
            
            feature_dict = {
                'length': len(text),
                'word_count': len(text.split()),
                'support_keyword_count': sum(1 for keyword in self.support_keywords if keyword in text_lower),
                'has_support_word': int('ì§€ì›' in text_lower),
                'has_business_word': int('ì‚¬ì—…' in text_lower),
                'has_startup_word': int(any(word in text_lower for word in ['ì°½ì—…', 'ìŠ¤íƒ€íŠ¸ì—…'])),
                'has_tech_word': int(any(word in text_lower for word in ['ê¸°ìˆ ', 'ê°œë°œ', 'ì—°êµ¬', 'r&d'])),
                'has_funding_word': int(any(word in text_lower for word in ['íˆ¬ìž', 'ìœµìž', 'ë³´ì¡°ê¸ˆ'])),
                'korean_ratio': len(re.findall(r'[ê°€-íž£]', text)) / max(len(text), 1),
                'number_count': len(re.findall(r'\d+', text))
            }
            
            features.append(list(feature_dict.values()))
        
        return np.array(features)
    
    def fit(self, texts):
        """íŠ¹ì„± ì¶”ì¶œê¸° í›ˆë ¨ (í›ˆë ¨ ë°ì´í„°ë¡œ TF-IDF í•™ìŠµ)"""
        print("ðŸ”§ íŠ¹ì„± ì¶”ì¶œê¸° í›ˆë ¨ ì¤‘...")
        self.tfidf.fit(texts)
        self.tfidf_fitted = True
        print("âœ… TF-IDF í›ˆë ¨ ì™„ë£Œ")
    
    def extract_features(self, texts, is_training=False):
        """ì „ì²´ íŠ¹ì„± ì¶”ì¶œ (Apple Silicon í˜¸í™˜)"""
        print("ðŸ” íŠ¹ì„± ì¶”ì¶œ ì‹œìž‘...")
        
        # TF-IDF íŠ¹ì„±
        print("ðŸ“Š TF-IDF íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        if is_training or not self.tfidf_fitted:
            # í›ˆë ¨ ì‹œì—ë§Œ fit_transform ì‚¬ìš©
            tfidf_features = self.tfidf.fit_transform(texts).toarray()
            self.tfidf_fitted = True
        else:
            # ì˜ˆì¸¡ ì‹œì—ëŠ” transformë§Œ ì‚¬ìš©
            tfidf_features = self.tfidf.transform(texts).toarray()
        
        # Sentence embedding íŠ¹ì„± (Apple Silicon í˜¸í™˜)
        if self.sentence_model:
            print("ðŸ§  Sentence Embedding íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
            try:
                # CPUë¡œ ë³€í™˜í•˜ì—¬ Apple Silicon í˜¸í™˜ì„± ë³´ìž¥
                sentence_features = []
                batch_size = 32
                
                for i in tqdm(range(0, len(texts), batch_size), desc="Sentence Embedding"):
                    batch_texts = texts[i:i+batch_size]
                    
                    # Apple Siliconì—ì„œëŠ” CPUë¡œ ì²˜ë¦¬
                    with torch.no_grad():
                        embeddings = self.sentence_model.encode(
                            batch_texts, 
                            convert_to_tensor=True,
                            device='cpu'  # Apple Silicon í˜¸í™˜ì„ ìœ„í•´ CPU ê°•ì œ ì‚¬ìš©
                        )
                        
                        if isinstance(embeddings, torch.Tensor):
                            embeddings = embeddings.cpu().numpy()
                        
                        sentence_features.extend(embeddings)
                
                sentence_features = np.array(sentence_features)
                print(f"âœ… Sentence Embedding í˜•íƒœ: {sentence_features.shape}")
                
            except Exception as e:
                print(f"âš ï¸ Sentence Embedding ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                sentence_features = np.zeros((len(texts), 384))  # ê¸°ë³¸ ì°¨ì›
        else:
            sentence_features = np.zeros((len(texts), 384))
        
        # ìˆ˜ë™ íŠ¹ì„±
        print("ðŸ”§ ìˆ˜ë™ íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        manual_features = self.extract_manual_features(texts)
        
        # íŠ¹ì„± ê²°í•©
        combined_features = np.hstack([
            tfidf_features,
            sentence_features, 
            manual_features
        ])
        
        print(f"âœ… ì „ì²´ íŠ¹ì„± í˜•íƒœ: {combined_features.shape}")
        return combined_features

# ============================================
# ì…€ 7: Apple Silicon í˜¸í™˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ í´ëž˜ìŠ¤ë“¤
# ============================================
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

class SupportProgramDataset(Dataset):
    """ì§€ì›ì‚¬ì—… ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹"""
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class DeepSupportClassifier(nn.Module):
    """ê³ ì„±ëŠ¥ ë‹¤ì¸µ ì‹ ê²½ë§ ë¶„ë¥˜ê¸° (8ì¸µ Deep Network)"""
    
    def __init__(self, input_dim, hidden_dims=[1024, 512, 256, 128, 64, 32], dropout_rate=0.3):
        super(DeepSupportClassifier, self).__init__()
        
        # 8ì¸µ ê¹Šì€ ì‹ ê²½ë§ êµ¬ì¡° ì„¤ê³„
        layers = []
        prev_dim = input_dim
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ (ìž…ë ¥ì¸µ â†’ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ)
        layers.append(nn.Linear(prev_dim, hidden_dims[0]))
        layers.append(nn.BatchNorm1d(hidden_dims[0]))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout_rate))
        prev_dim = hidden_dims[0]
        
        # ì¤‘ê°„ ì€ë‹‰ì¸µë“¤ (2~7ì¸µ)
        for hidden_dim in hidden_dims[1:]:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        
        # ì¶œë ¥ì¸µ (8ë²ˆì§¸ ì¸µ)
        layers.append(nn.Linear(prev_dim, 2))  # ì´ì§„ ë¶„ë¥˜
        
        self.network = nn.Sequential(*layers)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Xavier Uniform)
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

class AppleSiliconDeepLearningModel:
    """Apple Silicon í˜¸í™˜ ë”¥ëŸ¬ë‹ í”„ë¡œë•ì…˜ ëª¨ë¸"""
    
    def __init__(self, device='cpu'):
        self.device = device
        print(f"ðŸŽ Apple Silicon í˜¸í™˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {device})")
        
        self.feature_extractor = AppleSiliconFeatureExtractor(device=device)
        self.model = None
        self.is_fitted = False
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ê³ ì„±ëŠ¥ì„ ìœ„í•œ ì„¤ì •)
        self.learning_rate = 0.001
        self.batch_size = 32
        self.epochs = 100  # ë” ê¸´ í•™ìŠµ
        self.weight_decay = 1e-5
        
    def _create_model(self, input_dim):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„±"""
        model = DeepSupportClassifier(
            input_dim=input_dim,
            hidden_dims=[1024, 512, 256, 128, 64, 32],  # 6ê°œ ì€ë‹‰ì¸µ + ìž…ì¶œë ¥ì¸µ = 8ì¸µ
            dropout_rate=0.3
        )
        return model.to(self.device)
        
    def fit(self, texts, labels):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ (Apple Silicon í˜¸í™˜)"""
        print("ðŸ‹ï¸ Apple Silicon í˜¸í™˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì‹œìž‘...")
        print(f"ðŸ”¥ 8ì¸µ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ë¡œ {self.epochs} ì—í¬í¬ í›ˆë ¨")
        
        # íŠ¹ì„± ì¶”ì¶œ (í›ˆë ¨ ëª¨ë“œ)
        features = self.feature_extractor.extract_features(texts, is_training=True)
        input_dim = features.shape[1]
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„±
        self.model = self._create_model(input_dim)
        print(f"ðŸ§  ì‹ ê²½ë§ êµ¬ì¡°: ìž…ë ¥ì¸µ({input_dim}) â†’ 1024 â†’ 512 â†’ 256 â†’ 128 â†’ 64 â†’ 32 â†’ ì¶œë ¥ì¸µ(2)")
        
        # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
        dataset = SupportProgramDataset(features, labels)
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
        
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        
        # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤í•¨ìˆ˜ ì„¤ì •
        optimizer = optim.AdamW(self.model.parameters(), 
                               lr=self.learning_rate, 
                               weight_decay=self.weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', 
                                                        patience=5, factor=0.5)
        criterion = nn.CrossEntropyLoss()
        
        # í•™ìŠµ ê¸°ë¡
        train_losses = []
        val_losses = []
        val_accuracies = []
        
        print("ðŸš€ ë”¥ëŸ¬ë‹ í›ˆë ¨ ì‹œìž‘!")
        
        for epoch in range(self.epochs):
            # í›ˆë ¨ ë‹¨ê³„
            self.model.train()
            train_loss = 0
            train_samples = 0
            
            for batch_features, batch_labels in tqdm(train_loader, 
                                                   desc=f"Epoch {epoch+1}/{self.epochs}"):
                batch_features = batch_features.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_features)
                loss = criterion(outputs, batch_labels)
                loss.backward()
                
                # Gradient Clipping (ì•ˆì •ì ì¸ í•™ìŠµ)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += loss.item() * batch_features.size(0)
                train_samples += batch_features.size(0)
            
            # ê²€ì¦ ë‹¨ê³„
            self.model.eval()
            val_loss = 0
            val_correct = 0
            val_samples = 0
            
            with torch.no_grad():
                for batch_features, batch_labels in val_loader:
                    batch_features = batch_features.to(self.device)
                    batch_labels = batch_labels.to(self.device)
                    
                    outputs = self.model(batch_features)
                    loss = criterion(outputs, batch_labels)
                    
                    val_loss += loss.item() * batch_features.size(0)
                    _, predicted = torch.max(outputs.data, 1)
                    val_correct += (predicted == batch_labels).sum().item()
                    val_samples += batch_features.size(0)
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            avg_train_loss = train_loss / train_samples
            avg_val_loss = val_loss / val_samples
            val_accuracy = val_correct / val_samples
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            val_accuracies.append(val_accuracy)
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            scheduler.step(avg_val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(f"Epoch {epoch+1:3d}/{self.epochs} | "
                      f"Train Loss: {avg_train_loss:.4f} | "
                      f"Val Loss: {avg_val_loss:.4f} | "
                      f"Val Acc: {val_accuracy:.4f} | "
                      f"LR: {current_lr:.6f}")
        
        self.is_fitted = True
        
        # í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
        self._plot_training_history(train_losses, val_losses, val_accuracies)
        
        print(f"âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ! ìµœì¢… ê²€ì¦ ì •í™•ë„: {val_accuracies[-1]:.4f}")
        
    def _plot_training_history(self, train_losses, val_losses, val_accuracies):
        """í•™ìŠµ ê³¼ì • ì‹œê°í™”"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # ì†ì‹¤ í•¨ìˆ˜ ê·¸ëž˜í”„
        ax1.plot(train_losses, label='í›ˆë ¨ ì†ì‹¤', color='blue')
        ax1.plot(val_losses, label='ê²€ì¦ ì†ì‹¤', color='red')
        ax1.set_title('í•™ìŠµ ê³¼ì • - ì†ì‹¤ í•¨ìˆ˜')
        ax1.set_xlabel('ì—í¬í¬')
        ax1.set_ylabel('ì†ì‹¤')
        ax1.legend()
        ax1.grid(True)
        
        # ì •í™•ë„ ê·¸ëž˜í”„
        ax2.plot(val_accuracies, label='ê²€ì¦ ì •í™•ë„', color='green')
        ax2.set_title('í•™ìŠµ ê³¼ì • - ê²€ì¦ ì •í™•ë„')
        ax2.set_xlabel('ì—í¬í¬')
        ax2.set_ylabel('ì •í™•ë„')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.show()
        
    def predict(self, texts):
        """ì˜ˆì¸¡ (Apple Silicon í˜¸í™˜)"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        features = self.feature_extractor.extract_features(texts, is_training=False)
        features_tensor = torch.FloatTensor(features).to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(features_tensor)
            _, predicted = torch.max(outputs.data, 1)
            
        return predicted.cpu().numpy()
    
    def predict_proba(self, texts):
        """í™•ë¥  ì˜ˆì¸¡ (Apple Silicon í˜¸í™˜)"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        features = self.feature_extractor.extract_features(texts, is_training=False)
        features_tensor = torch.FloatTensor(features).to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(features_tensor)
            probabilities = F.softmax(outputs, dim=1)
            
        return probabilities.cpu().numpy()

# ============================================
# ì…€ 8: ë°ì´í„° ë¶„í•  ë° ëª¨ë¸ í›ˆë ¨
# ============================================
# í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸” ë¶„ë¦¬
texts = df['title'].tolist()
labels = df['label'].tolist()

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    texts, labels, test_size=0.2, random_state=42, stratify=labels
)

print(f"ðŸ“Š í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ")
print(f"ðŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ")

# Apple Silicon í˜¸í™˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
model = AppleSiliconDeepLearningModel(device=device)
model.fit(X_train, y_train)

# ============================================
# ì…€ 9: ëª¨ë¸ í‰ê°€
# ============================================
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)

# í‰ê°€ ì§€í‘œ
accuracy = accuracy_score(y_test, y_pred)
print(f"ðŸŽ¯ ì •í™•ë„: {accuracy:.4f}")

# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("\nðŸ“Š ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_pred, 
                          target_names=['ì¼ë°˜ í…ìŠ¤íŠ¸', 'ì§€ì›ì‚¬ì—…']))

# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['ì¼ë°˜ í…ìŠ¤íŠ¸', 'ì§€ì›ì‚¬ì—…'],
            yticklabels=['ì¼ë°˜ í…ìŠ¤íŠ¸', 'ì§€ì›ì‚¬ì—…'])
plt.title('í˜¼ë™ í–‰ë ¬ (Confusion Matrix)')
plt.ylabel('ì‹¤ì œ ë ˆì´ë¸”')
plt.xlabel('ì˜ˆì¸¡ ë ˆì´ë¸”')
plt.show()

# ============================================
# ì…€ 10: Apple Silicon í˜¸í™˜ ëª¨ë¸ ì €ìž¥
# ============================================
def save_apple_silicon_model(model, filepath):
    """Apple Silicon í˜¸í™˜ì„ ìœ„í•œ ëª¨ë¸ ì €ìž¥"""
    print(f"ðŸ’¾ Apple Silicon í˜¸í™˜ ëª¨ë¸ ì €ìž¥ ì¤‘: {filepath}")
    
    try:
        # ëª¨ë¸ ìƒíƒœë¥¼ CPUë¡œ ë³€í™˜í•˜ì—¬ ì €ìž¥
        model_data = {
            'model_state_dict': model.model.state_dict(),
            'model_structure': model.model,
            'feature_extractor': model.feature_extractor,
            'is_fitted': model.is_fitted,
            'device': 'cpu',  # Apple Silicon í˜¸í™˜ì„ ìœ„í•´ CPUë¡œ ì €ìž¥
            'model_type': 'AppleSiliconDeepLearningModel',
            'version': '1.0',
            'created_at': datetime.now().isoformat(),
            'training_accuracy': accuracy,
            'training_samples': len(X_train),
            'test_samples': len(X_test)
        }
        
        # pickle protocol 4ë¡œ ì €ìž¥ (Apple Silicon í˜¸í™˜)
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f, protocol=4)
        
        print(f"âœ… ëª¨ë¸ ì €ìž¥ ì™„ë£Œ: {filepath}")
        print(f"ðŸ“Š íŒŒì¼ í¬ê¸°: {os.path.getsize(filepath) / (1024*1024):.1f} MB")
        
        # ë©”íƒ€ë°ì´í„° ì €ìž¥
        metadata = {
            'model_type': 'AppleSiliconDeepLearningModel',
            'version': '1.0',
            'created_at': datetime.now().isoformat(),
            'accuracy': float(accuracy),
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'device_compatibility': ['cpu', 'mps', 'cuda'],
            'apple_silicon_optimized': True
        }
        
        metadata_path = filepath.replace('.pkl', '_metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ìž¥ ì™„ë£Œ: {metadata_path}")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì €ìž¥ ì‹¤íŒ¨: {e}")

# ëª¨ë¸ ì €ìž¥
import os
save_apple_silicon_model(model, 'apple_silicon_production_model.pkl')

# ============================================
# ì…€ 11: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
# ============================================
# ëª¨ë¸ í…ŒìŠ¤íŠ¸
test_texts = [
    "ì¤‘ì†Œê¸°ì—… ì°½ì—… ì§€ì› ì‚¬ì—…ì— ì°¸ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
    "ì˜¤ëŠ˜ ì¹œêµ¬ì™€ ë§›ìžˆëŠ” ì €ë…ì„ ë¨¹ì—ˆìŠµë‹ˆë‹¤",
    "ê¸°ìˆ ê°œë°œ R&D ì§€ì› í”„ë¡œê·¸ëž¨ ì•ˆë‚´",
    "ì£¼ë§ì— ì˜í™”ë¥¼ ë³´ëŸ¬ ê°ˆ ì˜ˆì •ìž…ë‹ˆë‹¤",
    "ë²¤ì²˜ê¸°ì—… íˆ¬ìž ìœ ì¹˜ ì§€ì›ì‚¬ì—…"
]

print("ðŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
predictions = model.predict(test_texts)
probabilities = model.predict_proba(test_texts)

for i, text in enumerate(test_texts):
    pred_label = "ì§€ì›ì‚¬ì—…" if predictions[i] == 1 else "ì¼ë°˜í…ìŠ¤íŠ¸"
    confidence = probabilities[i][predictions[i]]
    print(f"ðŸ“ '{text}' â†’ {pred_label} (ì‹ ë¢°ë„: {confidence:.3f})")

print("\nâœ… Apple Silicon í˜¸í™˜ ì½”ëž© í›ˆë ¨ ì½”ë“œ ì™„ë£Œ!")
print("ðŸŽ ì´ ëª¨ë¸ì€ Apple Silicon Macì—ì„œ ì™„ë²½í•˜ê²Œ ìž‘ë™í•©ë‹ˆë‹¤!")

# ============================================
# ì…€ 12: ë‹¤ìš´ë¡œë“œ ì¤€ë¹„
# ============================================
# ì½”ëž©ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
from google.colab import files

print("ðŸ“ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼ë“¤:")
download_files = [
    'apple_silicon_production_model.pkl',
    'apple_silicon_production_model_metadata.json',
    'training_data.json'
]

for file in download_files:
    if os.path.exists(file):
        print(f"âœ… {file} - {os.path.getsize(file) / (1024*1024):.1f} MB")
        files.download(file)
    else:
        print(f"âŒ {file} - íŒŒì¼ ì—†ìŒ")

print("\nðŸŽ‰ ëª¨ë“  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
print("ðŸŽ Apple Silicon Macì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!") 