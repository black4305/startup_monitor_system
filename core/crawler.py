"""
ì›¹ í¬ë¡¤ë§ ëª¨ë“ˆ - ì§€ì›ì‚¬ì—… ê³µê³  ìˆ˜ì§‘ (ê°•í™” ë²„ì „)
SSL, ì¸ì½”ë”©, User-Agent ë¬¸ì œ í•´ê²°
"""

import asyncio
import aiohttp
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import logging
from typing import List, Dict, Optional
from datetime import datetime
import traceback
import chardet
import ssl
import time

from .config import Config

logger = logging.getLogger(__name__)

class WebCrawler:
    """ì›¹ í¬ë¡¤ë§ ì—”ì§„ - ê°•í™” ë²„ì „"""
    
    def __init__(self, db_manager):
        self.db_manager = db_manager
        
        # ê¸°ë³¸ ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        
        # User-Agent ë¡œí…Œì´ì…˜ (ë´‡ ì°¨ë‹¨ ìš°íšŒ)
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0'
        ]
        self.current_ua_index = 0
        
        # í–¥ìƒëœ í—¤ë” ì„¤ì •
        self.update_session_headers()
        
        # SSL ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE
        
        # ì¸ì½”ë”© ì‹œë„ ìˆœì„œ
        self.encodings = ['utf-8', 'euc-kr', 'cp949', 'iso-8859-1']
        
        logger.info("ğŸŒ ì›¹ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def update_session_headers(self):
        """ì„¸ì…˜ í—¤ë” ì—…ë°ì´íŠ¸ (User-Agent ë¡œí…Œì´ì…˜)"""
        self.session.headers.update({
            'User-Agent': self.user_agents[self.current_ua_index],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1'
        })
        self.current_ua_index = (self.current_ua_index + 1) % len(self.user_agents)
    
    def get_page_with_fallback(self, url: str, site_name: str) -> Optional[BeautifulSoup]:
        """ê°•í™”ëœ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (SSL, ì¸ì½”ë”©, User-Agent ë¬¸ì œ í•´ê²°)"""
        logger.info(f"ğŸŒ í˜ì´ì§€ ì ‘ì† ì‹œë„: {site_name} - {url}")
        
        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
        methods = [
            self._get_with_ssl_verification,  # ê¸°ë³¸ SSL ê²€ì¦
            self._get_without_ssl_verification,  # SSL ê²€ì¦ ë¹„í™œì„±í™”
            self._get_with_different_ua,  # ë‹¤ë¥¸ User-Agent
        ]
        
        for i, method in enumerate(methods):
            try:
                logger.debug(f"   ì‹œë„ {i+1}: {method.__name__}")
                response = method(url)
                
                if response and response.status_code == 200:
                    # ì¸ì½”ë”© ê°ì§€ ë° ì²˜ë¦¬
                    soup = self._parse_with_encoding_detection(response)
                    if soup:
                        logger.info(f"âœ… {site_name} ì ‘ì† ì„±ê³µ (ë°©ë²• {i+1})")
                        return soup
                        
            except Exception as e:
                logger.debug(f"   ë°©ë²• {i+1} ì‹¤íŒ¨: {e}")
                continue
        
        logger.warning(f"âŒ {site_name} ëª¨ë“  ì ‘ì† ë°©ë²• ì‹¤íŒ¨")
        return None
    
    def _get_with_ssl_verification(self, url: str):
        """SSL ê²€ì¦ í™œì„±í™” ìƒíƒœë¡œ ìš”ì²­"""
        self.session.verify = True
        return self.session.get(url, timeout=Config.TIMEOUT)
    
    def _get_without_ssl_verification(self, url: str):
        """SSL ê²€ì¦ ë¹„í™œì„±í™” ìƒíƒœë¡œ ìš”ì²­"""
        self.session.verify = False
        # SSL ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        return self.session.get(url, timeout=Config.TIMEOUT)
    
    def _get_with_different_ua(self, url: str):
        """ë‹¤ë¥¸ User-Agentë¡œ ìš”ì²­"""
        self.update_session_headers()  # User-Agent ë³€ê²½
        self.session.verify = False  # SSLë„ ë¹„í™œì„±í™”
        return self.session.get(url, timeout=Config.TIMEOUT)
    
    def _parse_with_encoding_detection(self, response) -> Optional[BeautifulSoup]:
        """ì¸ì½”ë”© ê°ì§€ ë° íŒŒì‹±"""
        # 1ì°¨: Responseì˜ encoding ì‚¬ìš©
        if response.encoding and response.encoding.lower() != 'iso-8859-1':
            try:
                soup = BeautifulSoup(response.content, 'html.parser')
                if self._is_valid_korean_content(soup):
                    logger.debug(f"   ì¸ì½”ë”© ì„±ê³µ: {response.encoding}")
                    return soup
            except Exception as e:
                logger.debug(f"   Response encoding ì‹¤íŒ¨: {e}")
        
        # 2ì°¨: chardetë¡œ ì¸ì½”ë”© ê°ì§€
        try:
            detected = chardet.detect(response.content)
            if detected['encoding'] and detected['confidence'] > 0.7:
                decoded_content = response.content.decode(detected['encoding'])
                soup = BeautifulSoup(decoded_content, 'html.parser')
                if self._is_valid_korean_content(soup):
                    logger.debug(f"   ì¸ì½”ë”© ê°ì§€ ì„±ê³µ: {detected['encoding']} (ì‹ ë¢°ë„: {detected['confidence']:.2f})")
                    return soup
        except Exception as e:
            logger.debug(f"   Chardet ê°ì§€ ì‹¤íŒ¨: {e}")
        
        # 3ì°¨: ìˆœì°¨ì  ì¸ì½”ë”© ì‹œë„
        for encoding in self.encodings:
            try:
                decoded_content = response.content.decode(encoding, errors='ignore')
                soup = BeautifulSoup(decoded_content, 'html.parser')
                if self._is_valid_korean_content(soup):
                    logger.debug(f"   ì¸ì½”ë”© ì‹œë„ ì„±ê³µ: {encoding}")
                    return soup
            except Exception as e:
                logger.debug(f"   {encoding} ì‹œë„ ì‹¤íŒ¨: {e}")
                continue
        
        # 4ì°¨: ë§ˆì§€ë§‰ ì‹œë„ (ì˜¤ë¥˜ ë¬´ì‹œ)
        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            logger.debug(f"   ê¸°ë³¸ íŒŒì‹± ì‚¬ìš© (ì¸ì½”ë”© ë¬´ì‹œ)")
            return soup
        except Exception as e:
            logger.error(f"   ëª¨ë“  íŒŒì‹± ë°©ë²• ì‹¤íŒ¨: {e}")
            return None
    
    def _is_valid_korean_content(self, soup: BeautifulSoup) -> bool:
        """í•œêµ­ì–´ ì½˜í…ì¸  ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            text = soup.get_text()[:500]  # ì²˜ìŒ 500ìë§Œ í™•ì¸
            # í•œê¸€ ë¬¸ìê°€ ìˆê³ , ê¹¨ì§„ ë¬¸ìê°€ ì ì€ì§€ í™•ì¸
            korean_chars = len([c for c in text if 'ê°€' <= c <= 'í£'])
            broken_chars = text.count('')
            
            return korean_chars > 10 and broken_chars < 5
        except:
            return False
    
    def load_sites_from_supabase(self, priority: str = None, region: str = None) -> List[Dict]:
        """Supabaseì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ ë¡œë“œ"""
        try:
            return self.db_manager.get_crawling_sites(
                enabled_only=True, 
                priority=priority, 
                region=region
            )
        except Exception as e:
            logger.error(f"âŒ ì‚¬ì´íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    async def crawl_websites(self, max_sites: int = None, priority: str = None, 
                           region: str = None, concurrent_batch: int = 3, progress_callback=None) -> List[Dict]:
        """ë°°ì¹˜ ë³‘ë ¬ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (3ê°œì”© ë™ì‹œ ì²˜ë¦¬)"""
        sites = self.load_sites_from_supabase(priority, region)
        
        if max_sites:
            sites = sites[:max_sites]
            
        logger.info(f"ğŸ” {len(sites)}ê°œ ì‚¬ì´íŠ¸ì—ì„œ ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ (3ê°œì”© ë™ì‹œ)")
        
        results = []
        completed_count = 0
        
        # 3ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
        batch_size = 3
        for batch_start in range(0, len(sites), batch_size):
            batch_sites = sites[batch_start:batch_start + batch_size]
            
            logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_start//batch_size + 1}: {len(batch_sites)}ê°œ ì‚¬ì´íŠ¸ ë™ì‹œ ì²˜ë¦¬")
            
            # ë°°ì¹˜ ë‚´ ì‚¬ì´íŠ¸ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            batch_results = await self._process_batch_parallel(batch_sites, progress_callback, completed_count, len(sites))
            results.extend(batch_results)
            
            completed_count += len(batch_sites)
            
            # ë°°ì¹˜ ê°„ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if batch_start + batch_size < len(sites):
                logger.info("â±ï¸ ë°°ì¹˜ ê°„ 2ì´ˆ ëŒ€ê¸°...")
                await asyncio.sleep(2)
        
        # ìµœì¢… ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        if progress_callback:
            progress_callback(
                completed=len(sites), 
                total=len(sites), 
                current_site="",
                message="í¬ë¡¤ë§ ì™„ë£Œ"
            )
        
        logger.info(f"âœ… ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘")
        return results
    
    async def _process_batch_parallel(self, batch_sites: List[Dict], progress_callback, completed_so_far: int, total_sites: int) -> List[Dict]:
        """ë°°ì¹˜ ë‚´ ì‚¬ì´íŠ¸ë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬"""
        import concurrent.futures
        import time
        
        batch_results = []
        
        # ThreadPoolExecutorë¡œ ë™ì‹œ ì²˜ë¦¬ (3ê°œì”©) - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                # ê° ì‚¬ì´íŠ¸ë³„ Future ìƒì„±
                future_to_site = {}
                
                for i, site in enumerate(batch_sites):
                    site_name = site.get('name', '')
                    logger.info(f"ğŸ¯ ì‚¬ì´íŠ¸ {completed_so_far + i + 1}/{total_sites}: {site_name} (ë°°ì¹˜ ì²˜ë¦¬)")
                    
                    # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                    if progress_callback:
                        progress_callback(
                            completed=completed_so_far + i, 
                            total=total_sites, 
                            current_site=site_name,
                            message=f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {site_name}"
                        )
                    
                    # ë™ì‹œ ì‹¤í–‰ ì‹œì‘
                    future = executor.submit(self.crawl_single_site, site)
                    future_to_site[future] = site
                
                # ê²°ê³¼ ìˆ˜ì§‘ (ì™„ë£Œë˜ëŠ” ìˆœì„œëŒ€ë¡œ) - íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ ê°œì„ 
                try:
                    for future in concurrent.futures.as_completed(future_to_site, timeout=150):
                        site = future_to_site[future]
                        site_name = site.get('name', '')
                        
                        try:
                            site_programs = future.result(timeout=30)  # ê°œë³„ íƒ€ì„ì•„ì›ƒ
                            if site_programs:
                                batch_results.extend(site_programs)
                                logger.info(f"âœ… {site_name}: {len(site_programs)}ê°œ ìˆ˜ì§‘ (ë°°ì¹˜ ì™„ë£Œ)")
                            else:
                                logger.warning(f"âš ï¸ {site_name}: í”„ë¡œê·¸ë¨ ì—†ìŒ (ë°°ì¹˜ ì™„ë£Œ)")
                                
                        except concurrent.futures.TimeoutError:
                            logger.warning(f"âš ï¸ {site_name} íƒ€ì„ì•„ì›ƒ - ê±´ë„ˆëœ€")
                            future.cancel()
                        except Exception as e:
                            logger.error(f"âŒ {site_name} ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                            continue
                            
                except concurrent.futures.TimeoutError:
                    logger.error("âš ï¸ ë°°ì¹˜ ì „ì²´ íƒ€ì„ì•„ì›ƒ - ì™„ë£Œëœ ì‘ì—…ë§Œ ìˆ˜ì§‘")
                    # ì™„ë£Œëœ Futureë“¤ë§Œ ê²°ê³¼ ìˆ˜ì§‘
                    for future, site in future_to_site.items():
                        if future.done() and not future.cancelled():
                            try:
                                site_programs = future.result(timeout=1)
                                if site_programs:
                                    batch_results.extend(site_programs)
                                    logger.info(f"âœ… {site.get('name')}: {len(site_programs)}ê°œ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ êµ¬ì¡°)")
                            except:
                                pass
                                
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {e}")
            # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ìˆœì°¨ ì²˜ë¦¬ë¡œ í´ë°±
            logger.info("ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ë¡œ í´ë°±...")
            for site in batch_sites:
                site_name = site.get('name', '')
                try:
                    site_programs = self.crawl_single_site(site)
                    if site_programs:
                        batch_results.extend(site_programs)
                        logger.info(f"âœ… {site_name}: {len(site_programs)}ê°œ ìˆ˜ì§‘ (ìˆœì°¨ í´ë°±)")
                except Exception as site_e:
                    logger.error(f"âŒ {site_name} ìˆœì°¨ í¬ë¡¤ë§ë„ ì‹¤íŒ¨: {site_e}")
                    continue
        
        logger.info(f"ğŸ“¦ ë°°ì¹˜ ì™„ë£Œ: {len(batch_results)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘")
        return batch_results
    
    async def crawl_websites_streaming(self, max_sites: int = None, priority: str = None, 
                                     region: str = None, concurrent_batch: int = 3,
                                     program_callback=None, site_progress_callback=None):
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§ (í”„ë¡œê·¸ë¨ ë°œê²¬ ì¦‰ì‹œ ì½œë°± í˜¸ì¶œ)"""
        try:
            logger.info("ğŸ”„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§ ì‹œì‘")
            
            # ì‚¬ì´íŠ¸ ëª©ë¡ ë¡œë“œ
            sites = self.load_sites_from_supabase(priority=priority, region=region)
            if max_sites:
                sites = sites[:max_sites]
            
            logger.info(f"ğŸ“‹ ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§ ëŒ€ìƒ: {len(sites)}ê°œ ì‚¬ì´íŠ¸")
            
            completed_sites = 0
            total_programs_found = 0
            
            # ë°°ì¹˜ë³„ ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§
            for i in range(0, len(sites), concurrent_batch):
                batch_sites = sites[i:i + concurrent_batch]
                
                logger.info(f"ğŸ“¦ ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ {i//concurrent_batch + 1}: {len(batch_sites)}ê°œ ì‚¬ì´íŠ¸ ì²˜ë¦¬")
                
                # ë°°ì¹˜ ë‚´ ì‚¬ì´íŠ¸ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ë©´ì„œ ì‹¤ì‹œê°„ ì½œë°±
                batch_results = await self._process_batch_streaming(
                    batch_sites, program_callback, site_progress_callback, 
                    completed_sites, len(sites)
                )
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                for site_programs_count in batch_results:
                    total_programs_found += site_programs_count
                
                completed_sites += len(batch_results)
                
                # ë°°ì¹˜ ê°„ ì§€ì—°
                if i + concurrent_batch < len(sites):
                    logger.info("â±ï¸ ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ê°„ 1ì´ˆ ëŒ€ê¸°...")
                    await asyncio.sleep(1)
            
            logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§ ì™„ë£Œ: {completed_sites}ê°œ ì‚¬ì´íŠ¸ì—ì„œ ì´ {total_programs_found}ê°œ í”„ë¡œê·¸ë¨ ì²˜ë¦¬")
            
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise
    
    async def _process_batch_streaming(self, batch_sites: List[Dict], program_callback, 
                                     site_progress_callback, completed_so_far: int, total_sites: int) -> List[int]:
        """ë°°ì¹˜ ë‚´ ì‚¬ì´íŠ¸ë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬í•˜ë©´ì„œ í”„ë¡œê·¸ë¨ ë°œê²¬ ì¦‰ì‹œ ì½œë°± í˜¸ì¶œ"""
        import concurrent.futures
        
        batch_results = []
        
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                # ê° ì‚¬ì´íŠ¸ë³„ Future ìƒì„±
                future_to_site = {}
                
                for i, site in enumerate(batch_sites):
                    site_name = site.get('name', '')
                    logger.info(f"ğŸ¯ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ì´íŠ¸ {completed_so_far + i + 1}/{total_sites}: {site_name}")
                    
                    # ìŠ¤íŠ¸ë¦¬ë°ìš© ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰
                    future = executor.submit(self._crawl_single_site_streaming, site, program_callback)
                    future_to_site[future] = site
                
                # ê²°ê³¼ ìˆ˜ì§‘ (ì™„ë£Œë˜ëŠ” ìˆœì„œëŒ€ë¡œ)
                try:
                    for future in concurrent.futures.as_completed(future_to_site, timeout=120):
                        site = future_to_site[future]
                        site_name = site.get('name', '')
                        
                        try:
                            programs_count = future.result(timeout=30)
                            batch_results.append(programs_count)
                            
                            # ì‚¬ì´íŠ¸ ì™„ë£Œ ì§„í–‰ ìƒí™© ì½œë°±
                            if site_progress_callback:
                                await site_progress_callback(
                                    site_info=site,
                                    programs_found=programs_count,
                                    completed_sites=completed_so_far + len(batch_results),
                                    total_sites=total_sites
                                )
                            
                            logger.info(f"âœ… {site_name}: {programs_count}ê°œ ì²˜ë¦¬ ì™„ë£Œ (ìŠ¤íŠ¸ë¦¬ë°)")
                            
                        except concurrent.futures.TimeoutError:
                            logger.warning(f"âš ï¸ {site_name} ìŠ¤íŠ¸ë¦¬ë° íƒ€ì„ì•„ì›ƒ - ê±´ë„ˆëœ€")
                            batch_results.append(0)
                            future.cancel()
                        except Exception as e:
                            logger.error(f"âŒ {site_name} ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                            batch_results.append(0)
                            continue
                            
                except concurrent.futures.TimeoutError:
                    logger.error("âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ì „ì²´ íƒ€ì„ì•„ì›ƒ")
                    # ë¯¸ì™„ë£Œ ì‚¬ì´íŠ¸ë“¤ì€ 0ìœ¼ë¡œ ì²˜ë¦¬
                    while len(batch_results) < len(batch_sites):
                        batch_results.append(0)
                        
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨ ì‹œ 0ìœ¼ë¡œ ì±„ìš°ê¸°
            while len(batch_results) < len(batch_sites):
                batch_results.append(0)
        
        logger.info(f"ğŸ“¦ ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ì™„ë£Œ: {sum(batch_results)}ê°œ í”„ë¡œê·¸ë¨ ì²˜ë¦¬")
        return batch_results
    
    def _crawl_single_site_streaming(self, site_info: Dict, program_callback) -> int:
        """ë‹¨ì¼ ì‚¬ì´íŠ¸ ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§ (í”„ë¡œê·¸ë¨ ë°œê²¬ ì¦‰ì‹œ ì½œë°±)"""
        site_name = site_info.get('name', '')
        programs_count = 0
        
        try:
            logger.info(f"ğŸ” ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§: {site_name}")
            
            # ê¸°ì¡´ í¬ë¡¤ë§ ë¡œì§ ì‚¬ìš©
            programs = self.crawl_single_site(site_info)
            
            if programs and program_callback:
                # ë°œê²¬ëœ ê° í”„ë¡œê·¸ë¨ì„ ì¦‰ì‹œ ì½œë°±ìœ¼ë¡œ ì „ë‹¬
                for program_data in programs:
                    try:
                        # ë¹„ë™ê¸° ì½œë°±ì„ ë™ê¸° í™˜ê²½ì—ì„œ ì‹¤í–‰
                        import asyncio
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        
                        # ì½œë°± ì‹¤í–‰ (AI ë¶„ì„ + DB ì €ì¥)
                        success = loop.run_until_complete(
                            program_callback(program_data, site_info)
                        )
                        
                        if success:
                            programs_count += 1
                            logger.debug(f"ğŸ“¤ í”„ë¡œê·¸ë¨ ìŠ¤íŠ¸ë¦¬ë° ì„±ê³µ: '{program_data.get('title', '')[:30]}...'")
                        else:
                            logger.debug(f"ğŸ“¤ í”„ë¡œê·¸ë¨ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: '{program_data.get('title', '')[:30]}...'")
                            
                        loop.close()
                        
                    except Exception as callback_error:
                        logger.error(f"âŒ í”„ë¡œê·¸ë¨ ì½œë°± ì‹¤íŒ¨: {callback_error}")
                        continue
            
            logger.info(f"âœ… {site_name}: {programs_count}ê°œ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
            return programs_count
            
        except Exception as e:
            logger.error(f"âŒ {site_name} ìŠ¤íŠ¸ë¦¬ë° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return 0
    
    async def crawl_single_site_async(self, session: aiohttp.ClientSession, 
                                    semaphore: asyncio.Semaphore, site_info: Dict) -> List[Dict]:
        """ë‹¨ì¼ ì‚¬ì´íŠ¸ ë¹„ë™ê¸° í¬ë¡¤ë§ (ê°•í™” ë²„ì „)"""
        async with semaphore:
            try:
                site_id = site_info.get('id', '')
                site_name = site_info.get('name', '')
                base_url = site_info.get('url', '')
                
                logger.info(f"ğŸ” ê°•í™”ëœ í¬ë¡¤ë§: {site_name}")
                
                # SSL ê²€ì¦ ë¹„í™œì„±í™”ëœ ì»¤ë„¥í„° ì‚¬ìš©
                connector = aiohttp.TCPConnector(ssl=False)
                timeout = aiohttp.ClientTimeout(total=Config.TIMEOUT)
                
                async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                    # í—¤ë” ì„¤ì •
                    headers = {
                        'User-Agent': self.user_agents[0],
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                    }
                    
                    async with session.get(base_url, headers=headers) as response:
                        if response.status != 200:
                            logger.warning(f"âš ï¸ {site_name} ì ‘ì† ì‹¤íŒ¨: {response.status}")
                            return []
                        
                        html_content = await response.read()
                        
                        # ì¸ì½”ë”© ì²˜ë¦¬
                        soup = self._parse_response_content(html_content)
                        if not soup:
                            logger.warning(f"âš ï¸ {site_name} íŒŒì‹± ì‹¤íŒ¨")
                            return []
                        
                        programs = self.extract_announcements_from_site(soup, site_info)
                        
                        # í¬ë¡¤ë§ í†µê³„ ì—…ë°ì´íŠ¸
                        self.db_manager.update_crawling_stats(site_id, True, {
                            'programs_found': len(programs),
                            'crawled_at': datetime.now().isoformat()
                        })
                        
                        logger.info(f"âœ… {site_name}: {len(programs)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘")
                        return programs
                        
            except Exception as e:
                logger.error(f"âŒ {site_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                self.db_manager.update_crawling_stats(site_id, False, {
                    'error': str(e),
                    'crawled_at': datetime.now().isoformat()
                })
                return []
    
    def _parse_response_content(self, content: bytes) -> Optional[BeautifulSoup]:
        """Response ë‚´ìš© íŒŒì‹± (ì¸ì½”ë”© ê°ì§€)"""
        # chardetë¡œ ì¸ì½”ë”© ê°ì§€
        try:
            detected = chardet.detect(content)
            if detected['encoding'] and detected['confidence'] > 0.7:
                decoded_content = content.decode(detected['encoding'])
                return BeautifulSoup(decoded_content, 'html.parser')
        except:
            pass
        
        # ìˆœì°¨ì  ì¸ì½”ë”© ì‹œë„
        for encoding in self.encodings:
            try:
                decoded_content = content.decode(encoding, errors='ignore')
                return BeautifulSoup(decoded_content, 'html.parser')
            except:
                continue
        
        return None
    
    def crawl_single_site(self, site_info: Dict) -> List[Dict]:
        """ë‹¨ì¼ ì‚¬ì´íŠ¸ ë™ê¸° í¬ë¡¤ë§ (ê°•í™” ë²„ì „)"""
        site_name = site_info.get('name', '')
        base_url = site_info.get('url', '')
        
        logger.info(f"ğŸ” ê°•í™”ëœ í¬ë¡¤ë§: {site_name}")
        
        try:
            # ê°•í™”ëœ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            soup = self.get_page_with_fallback(base_url, site_name)
            
            if not soup:
                logger.warning(f"âš ï¸ {site_name} í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
                return []
            
            programs = self.extract_announcements_from_site(soup, site_info)
            logger.info(f"âœ… {site_name}: {len(programs)}ê°œ ìˆ˜ì§‘")
            return programs
            
        except Exception as e:
            logger.error(f"âŒ {site_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def extract_announcements_from_site(self, soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
        """ì‚¬ì´íŠ¸ì—ì„œ ê³µê³  ì¶”ì¶œ - ë‹¤ì¤‘ í˜ì´ì§€ ì§€ì› ë²„ì „"""
        site_name = site_info.get('name', '')
        base_url = site_info.get('url', '')
        
        programs = []
        logger.info(f"ğŸ” {site_name} ì—ì„œ ê³µê³  ì¶”ì¶œ ì‹œì‘")
        
        # í˜„ì¬ í˜ì´ì§€ì—ì„œ í”„ë¡œê·¸ë¨ ì¶”ì¶œ
        current_page_programs = self._extract_programs_from_page(soup, site_info, base_url)
        programs.extend(current_page_programs)
        
        # ì—¬ëŸ¬ í˜ì´ì§€ ì²˜ë¦¬ (ìµœëŒ€ 3í˜ì´ì§€ê¹Œì§€)
        max_pages = 3
        page_count = 1
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ ì°¾ê¸°
        pagination_links = self._find_pagination_links(soup, base_url)
        
        for page_url in pagination_links[:max_pages-1]:  # í˜„ì¬ í˜ì´ì§€ ì œì™¸í•˜ê³  2í˜ì´ì§€ ë”
            try:
                logger.info(f"ğŸ“„ {site_name} {page_count+1}í˜ì´ì§€ í¬ë¡¤ë§: {page_url}")
                
                page_soup = self.get_page_with_fallback(page_url, site_name)
                if page_soup:
                    page_programs = self._extract_programs_from_page(page_soup, site_info, page_url)
                    programs.extend(page_programs)
                    page_count += 1
                    
                    # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                    time.sleep(1)
                else:
                    logger.warning(f"âš ï¸ {site_name} {page_count+1}í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {site_name} {page_count+1}í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_programs = []
        seen_urls = set()
        
        for program in programs:
            if program['url'] not in seen_urls:
                unique_programs.append(program)
                seen_urls.add(program['url'])
        
        logger.info(f"âœ… {site_name}: ì´ {len(unique_programs)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘ ({page_count}í˜ì´ì§€)")
        return unique_programs[:Config.MAX_PROGRAMS_PER_SITE]
    
    def _find_pagination_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ ì°¾ê¸°"""
        pagination_links = []
        
        # ë‹¤ì–‘í•œ í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ ì‹œë„
        pagination_selectors = [
            # ì¼ë°˜ì ì¸ í˜ì´ì§€ë„¤ì´ì…˜
            '.pagination a', '.paging a', '.page-nav a',
            # ë²ˆí˜¸ ê¸°ë°˜
            'a[href*="page="]', 'a[href*="pageNo="]', 'a[href*="p="]',
            # ë‹¤ìŒ í˜ì´ì§€
            'a[href*="next"]', '.next a', '.btn-next',
            # í•œêµ­ì–´ ë‹¤ìŒ
            'a:contains("ë‹¤ìŒ")', 'a:contains(">")', 'a:contains("â–¶")'
        ]
        
        for selector in pagination_selectors:
            try:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href')
                    if href and href not in pagination_links:
                        full_url = urljoin(base_url, href)
                        pagination_links.append(full_url)
                        
                        # ìµœëŒ€ 5ê°œ ë§í¬ê¹Œì§€ë§Œ
                        if len(pagination_links) >= 5:
                            break
                            
                if pagination_links:
                    break  # í•˜ë‚˜ì˜ íŒ¨í„´ì—ì„œ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
                    
            except Exception as e:
                continue
        
        return pagination_links
    
    def _extract_programs_from_page(self, soup: BeautifulSoup, site_info: Dict, page_url: str) -> List[Dict]:
        """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ í”„ë¡œê·¸ë¨ ì¶”ì¶œ"""
        site_name = site_info.get('name', '')
        programs = []
        
        # 1ë‹¨ê³„: ëª¨ë“  ë§í¬ì—ì„œ ì§€ì›ì‚¬ì—… ê´€ë ¨ ë§í¬ ì°¾ê¸°
        all_links = soup.find_all('a', href=True)
        
        # ì§€ì›ì‚¬ì—… ê´€ë ¨ í‚¤ì›Œë“œ (URL ë° í…ìŠ¤íŠ¸)
        support_keywords = [
            # í•œêµ­ì–´ í‚¤ì›Œë“œ
            'ê³µê³ ', 'ì§€ì›', 'ì‚¬ì—…', 'ëª¨ì§‘', 'ì‹ ì²­', 'ì„ ì •', 'ì°½ì—…', 'ë²¤ì²˜', 
            'ìŠ¤íƒ€íŠ¸ì—…', 'ê¸°ì—…', 'íˆ¬ì', 'ìê¸ˆ', 'ë³´ì¡°ê¸ˆ', 'ìœµì', 'r&d', 'ì—°êµ¬', 'ê°œë°œ',
            # ì˜ì–´ í‚¤ì›Œë“œ  
            'notice', 'support', 'business', 'program', 'startup', 'venture',
            'funding', 'grant', 'investment', 'announce', 'call', 'apply'
        ]
        
        found_links = []
        for link in all_links:
            href = link.get('href', '').lower()
            text = link.get_text(strip=True).lower()
            
            # URLì´ë‚˜ í…ìŠ¤íŠ¸ì— ì§€ì›ì‚¬ì—… í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
            if any(keyword in href for keyword in support_keywords) or \
               any(keyword in text for keyword in support_keywords):
                found_links.append(link)
        
        # ì¤‘ë³µ ì œê±°
        unique_links = {}
        for link in found_links:
            href = link.get('href', '')
            if href and href not in unique_links:
                unique_links[href] = link
        
        for href, link in list(unique_links.items()):
            try:
                # ìƒëŒ€ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
                full_url = urljoin(page_url, href)
                title = link.get_text(strip=True)
                
                if self.is_support_program_title(title):
                    # ê°„ë‹¨í•œ ë‚´ìš© ì¶”ì¶œ
                    content = self.extract_brief_content(full_url, title)
                    
                    program = {
                        'title': title,
                        'content': content,
                        'url': full_url,
                        'site_name': site_name,
                        'extracted_at': datetime.now().isoformat()
                    }
                    programs.append(program)
                    
            except Exception as e:
                logger.warning(f"ë§í¬ ì²˜ë¦¬ ì‹¤íŒ¨ {href}: {e}")
                continue
        
        return programs
    
    def extract_brief_content(self, url: str, title: str) -> str:
        """URLì—ì„œ ìƒì„¸í•œ ê³µê³  ë‚´ìš© ì¶”ì¶œ (AI ë¶„ì„ìš© - ê°•í™” ë²„ì „)"""
        try:
            # ìƒì„¸ í˜ì´ì§€ ì ‘ì†
            page_soup = self.get_page_with_fallback(url, title[:20])
            if not page_soup:
                logger.warning(f"âš ï¸ ìƒì„¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {url}")
                return "ìƒì„¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for unwanted in page_soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'comment']):
                unwanted.decompose()
            
            extracted_content = []
            
            # 1ë‹¨ê³„: ì œëª© ë‹¤ì‹œ ì¶”ì¶œ (í˜ì´ì§€ ë‚´ ì œëª©ì´ ë” ì •í™•í•  ìˆ˜ ìˆìŒ)
            title_selectors = [
                'h1', 'h2', '.title', '.subject', '.board-title', '.notice-title',
                '.view-title', '.content-title', '.detail-title', 'article h1'
            ]
            for selector in title_selectors:
                title_elem = page_soup.select_one(selector)
                if title_elem and len(title_elem.get_text().strip()) > 5:
                    page_title = title_elem.get_text().strip()
                    if page_title != title and len(page_title) > 10:  # ê¸°ì¡´ ì œëª©ê³¼ ë‹¤ë¥´ê³  ì¶©ë¶„íˆ ê¸¸ë©´
                        extracted_content.append(f"ğŸ“‹ ìƒì„¸ì œëª©: {page_title}")
                    break
            
            # 2ë‹¨ê³„: ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (ë” í¬ê´„ì ì¸ ì„ íƒì)
            content_selectors = [
                # ì¼ë°˜ì ì¸ ì½˜í…ì¸  ì„ íƒì
                '.content', '.board-content', '.notice-content', '.view-content',
                '#content', '#board_content', '#notice_content', '#view_content',
                '.post-content', '.article-content', '.text-content', '.detail-content',
                
                # í…Œì´ë¸” ê¸°ë°˜ ì½˜í…ì¸ 
                '.board-view', '.view-body', '.content-body', '.detail-body',
                'table.view', 'table.board', '.board_view', '#board_view',
                
                # ê¸°íƒ€ ê°€ëŠ¥í•œ ì„ íƒì
                'article', '.article', 'main', '.main', '#main',
                '.container .content', '.wrapper .content', '.board-wrap'
            ]
            
            main_content = ""
            for selector in content_selectors:
                content_elem = page_soup.select_one(selector)
                if content_elem:
                    # í…Œì´ë¸”ì´ë©´ êµ¬ì¡°ì ìœ¼ë¡œ íŒŒì‹±
                    if content_elem.name == 'table' or content_elem.find('table'):
                        main_content = self._parse_table_content(content_elem)
                    else:
                        main_content = content_elem.get_text(separator='\n', strip=True)
                    
                    if len(main_content) > 100:  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´
                        break
            
            # 3ë‹¨ê³„: ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ bodyì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            if not main_content or len(main_content) < 200:
                body_text = page_soup.get_text(separator='\n', strip=True)
                main_content = self._extract_relevant_content(body_text)
            
            if main_content:
                # í•µì‹¬ ì •ë³´ ì¶”ì¶œì„ ìœ„í•œ êµ¬ì¡°í™”ëœ íŒŒì‹±
                lines = main_content.split('\n')
                important_info = {}
                
                # ì¤‘ìš”í•œ ì •ë³´ íŒ¨í„´
                info_patterns = {
                    'ì§€ì›ê¸ˆì•¡': r'(ì§€ì›ê¸ˆì•¡|ì§€ì›ê·œëª¨|ì‚¬ì—…ë¹„|ì§€ì›í•œë„)[\s:ï¼š]*(.*?)(?:\n|$)',
                    'ì§€ì›ëŒ€ìƒ': r'(ì§€ì›ëŒ€ìƒ|ì‹ ì²­ìê²©|ëŒ€ìƒê¸°ì—…|ì°¸ê°€ìê²©)[\s:ï¼š]*(.*?)(?:\n|$)',
                    'ì‹ ì²­ê¸°ê°„': r'(ì‹ ì²­ê¸°ê°„|ì ‘ìˆ˜ê¸°ê°„|ëª¨ì§‘ê¸°ê°„|ê³µê³ ê¸°ê°„)[\s:ï¼š]*(.*?)(?:\n|$)',
                    'ì§€ì›ë‚´ìš©': r'(ì§€ì›ë‚´ìš©|ì‚¬ì—…ë‚´ìš©|ì£¼ìš”ë‚´ìš©|ì§€ì›ì‚¬í•­)[\s:ï¼š]*(.*?)(?:\n|$)',
                    'ì„ ì •ê¸°ì¤€': r'(ì„ ì •ê¸°ì¤€|í‰ê°€ê¸°ì¤€|ì‹¬ì‚¬ê¸°ì¤€|ì„ ë°œê¸°ì¤€)[\s:ï¼š]*(.*?)(?:\n|$)'
                }
                
                import re
                for key, pattern in info_patterns.items():
                    match = re.search(pattern, main_content, re.MULTILINE | re.DOTALL)
                    if match:
                        value = match.group(2).strip()
                        if value and len(value) > 5:
                            important_info[key] = value[:300]  # ìµœëŒ€ 300ì
                
                # êµ¬ì¡°í™”ëœ ë‚´ìš© ìƒì„±
                if important_info:
                    for key, value in important_info.items():
                        extracted_content.append(f"\nğŸ”¸ {key}: {value}")
                
                # ê¸ˆì•¡ ì •ë³´ íŠ¹ë³„ ì¶”ì¶œ
                money_matches = re.findall(r'(\d+(?:,\d+)*(?:\.\d+)?\s*(?:ì–µì›|ì²œë§Œì›|ë°±ë§Œì›|ë§Œì›|ì›))', main_content)
                if money_matches:
                    extracted_content.append(f"\nğŸ’° ê¸ˆì•¡ì •ë³´: {', '.join(set(money_matches[:5]))}")
                
                # ë‚ ì§œ ì •ë³´ íŠ¹ë³„ ì¶”ì¶œ
                date_matches = re.findall(r'(\d{4}[\së…„.-]\d{1,2}[\sì›”.-]\d{1,2}[\sì¼]?)', main_content)
                if date_matches:
                    extracted_content.append(f"\nğŸ“… ì¼ì •ì •ë³´: {', '.join(set(date_matches[:5]))}")
                
                # ì „ì²´ ë‚´ìš© ìš”ì•½ (ìµœëŒ€ 1500ì)
                if len(extracted_content) < 3:  # êµ¬ì¡°í™”ëœ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´
                    summary = self._summarize_content(main_content, 1500)
                    extracted_content.append(f"\nğŸ“„ ë‚´ìš©ìš”ì•½:\n{summary}")
            
            # ìµœì¢… ì»¨í…ì¸  ì¡°í•©
            if extracted_content:
                return '\n'.join(extracted_content)
            else:
                return f"ì œëª©: {title}\n(ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨)"
                
        except Exception as e:
            logger.error(f"âŒ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return f"ì œëª©: {title}\n(ìƒì„¸ ë‚´ìš© ë¡œë“œ ì˜¤ë¥˜)"
    
    def is_support_program_title(self, title: str) -> bool:
        """ì œëª©ì´ ì§€ì›ì‚¬ì—…ì¸ì§€ íŒë‹¨"""
        if not title or len(title.strip()) < 5:
            return False
        
        # ì§€ì›ì‚¬ì—… ê´€ë ¨ í‚¤ì›Œë“œ
        support_keywords = [
            'ì§€ì›', 'ì‚¬ì—…', 'ê³µê³ ', 'ëª¨ì§‘', 'ì„ ì •', 'ì‹ ì²­',
            'ì°½ì—…', 'ë²¤ì²˜', 'ìŠ¤íƒ€íŠ¸ì—…', 'ê¸°ì—…', 'íˆ¬ì',
            'R&D', 'ì—°êµ¬', 'ê°œë°œ', 'ê¸°ìˆ ', 'í˜ì‹ '
        ]
        
        # ì œì™¸í•  í‚¤ì›Œë“œ
        exclude_keywords = [
            'ì±„ìš©', 'êµ¬ì¸', 'êµ¬ì§', 'ì…ì°°', 'ê³µì‚¬',
            'í–‰ì‚¬', 'ì„¸ë¯¸ë‚˜', 'ê°•ì˜', 'êµìœ¡'
        ]
        
        title_lower = title.lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        for keyword in exclude_keywords:
            if keyword in title_lower:
                return False
        
        # ì§€ì›ì‚¬ì—… í‚¤ì›Œë“œ ì²´í¬
        for keyword in support_keywords:
            if keyword in title_lower:
                return True
        
        return False
    
    def _parse_table_content(self, table_elem) -> str:
        """í…Œì´ë¸” í˜•ì‹ì˜ ì½˜í…ì¸ ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ íŒŒì‹±"""
        content_lines = []
        
        # í…Œì´ë¸”ì˜ ëª¨ë“  í–‰ ì²˜ë¦¬
        rows = table_elem.find_all('tr')
        for row in rows:
            # thì™€ td ìš”ì†Œ ì¶”ì¶œ
            headers = row.find_all('th')
            cells = row.find_all('td')
            
            # í—¤ë”ê°€ ìˆìœ¼ë©´ í—¤ë”: ê°’ í˜•ì‹ìœ¼ë¡œ
            if headers and cells:
                for i, header in enumerate(headers):
                    if i < len(cells):
                        header_text = header.get_text(strip=True)
                        cell_text = cells[i].get_text(strip=True)
                        if header_text and cell_text:
                            content_lines.append(f"{header_text}: {cell_text}")
            # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì…€ ë‚´ìš©ë§Œ
            elif cells:
                row_text = ' | '.join([cell.get_text(strip=True) for cell in cells])
                if row_text.strip():
                    content_lines.append(row_text)
        
        return '\n'.join(content_lines)
    
    def _extract_relevant_content(self, full_text: str) -> str:
        """ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì§€ì›ì‚¬ì—… ê´€ë ¨ ë‚´ìš©ë§Œ ì¶”ì¶œ"""
        lines = full_text.split('\n')
        relevant_lines = []
        
        # ê´€ë ¨ í‚¤ì›Œë“œ
        relevant_keywords = [
            'ì§€ì›', 'ì‚¬ì—…', 'ê³µê³ ', 'ëª¨ì§‘', 'ì‹ ì²­', 'ëŒ€ìƒ', 'ìê²©', 'ê¸°ê°„',
            'ê¸ˆì•¡', 'ê·œëª¨', 'ë‚´ìš©', 'ì¡°ê±´', 'ì„ ì •', 'í‰ê°€', 'ì œì¶œ', 'ì„œë¥˜',
            'ì°½ì—…', 'ìŠ¤íƒ€íŠ¸ì—…', 'ë²¤ì²˜', 'ê¸°ì—…', 'íˆ¬ì', 'ìœµì', 'ë³´ì¡°ê¸ˆ',
            'ì–µì›', 'ì²œë§Œì›', 'ë°±ë§Œì›', 'ë§Œì›', 'ê¸°í•œ', 'ë§ˆê°', 'ì ‘ìˆ˜'
        ]
        
        # ê° ì¤„ì„ ê²€ì‚¬í•˜ì—¬ ê´€ë ¨ ë‚´ìš©ë§Œ ì¶”ì¶œ
        for line in lines:
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in relevant_keywords):
                relevant_lines.append(line.strip())
                
                # ì¶©ë¶„í•œ ë‚´ìš©ì„ ìˆ˜ì§‘í–ˆìœ¼ë©´ ì¤‘ë‹¨
                if len('\n'.join(relevant_lines)) > 1000:
                    break
        
        return '\n'.join(relevant_lines)
    
    def _summarize_content(self, content: str, max_length: int = 1500) -> str:
        """ê¸´ ì½˜í…ì¸ ë¥¼ ìš”ì•½"""
        if len(content) <= max_length:
            return content
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = content.replace('ã€‚', '.').split('.')
        
        # ì¤‘ìš”í•œ ë¬¸ì¥ ìš°ì„  ì„ íƒ
        important_sentences = []
        other_sentences = []
        
        important_keywords = [
            'ì§€ì›ê¸ˆì•¡', 'ì§€ì›ê·œëª¨', 'ì§€ì›ëŒ€ìƒ', 'ì‹ ì²­ê¸°ê°„', 'ë§ˆê°ì¼',
            'ì„ ì •ê¸°ì¤€', 'í‰ê°€ê¸°ì¤€', 'ì§€ì›ë‚´ìš©', 'ì‚¬ì—…ë‚´ìš©', 'ì§€ì›ì¡°ê±´',
            'ì–µì›', 'ì²œë§Œì›', 'ë°±ë§Œì›', 'ë§Œì›'
        ]
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # ì¤‘ìš” í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë¬¸ì¥ ìš°ì„ 
            if any(keyword in sentence for keyword in important_keywords):
                important_sentences.append(sentence)
            else:
                other_sentences.append(sentence)
        
        # ì¤‘ìš” ë¬¸ì¥ë¶€í„° ì±„ìš°ê³ , ë‚¨ì€ ê³µê°„ì— ë‹¤ë¥¸ ë¬¸ì¥ ì¶”ê°€
        result = []
        current_length = 0
        
        # ì¤‘ìš” ë¬¸ì¥ ì¶”ê°€
        for sentence in important_sentences:
            if current_length + len(sentence) < max_length:
                result.append(sentence)
                current_length += len(sentence) + 1
        
        # ë‚˜ë¨¸ì§€ ë¬¸ì¥ ì¶”ê°€
        for sentence in other_sentences:
            if current_length + len(sentence) < max_length:
                result.append(sentence)
                current_length += len(sentence) + 1
            else:
                break
        
        return '. '.join(result) + '...' 