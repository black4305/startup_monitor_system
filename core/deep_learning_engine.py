#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸ§  ê³ ì„±ëŠ¥ ë”¥ëŸ¬ë‹ ì—”ì§„
ì½”ëž©ì—ì„œ í›ˆë ¨í•œ 8ì¸µ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
import pickle
import json
import re
from datetime import datetime
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import logging
from typing import List, Dict, Optional, Tuple, Any
import glob
import os
import sys

# ============================================
# ì»¤ìŠ¤í…€ Unpickler (ëª¨ë“ˆ ê²½ë¡œ ë¬¸ì œ í•´ê²°)
# ============================================

class CustomUnpickler(pickle.Unpickler):
    """ëª¨ë“ˆ ê²½ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì»¤ìŠ¤í…€ unpickler"""
    
    def find_class(self, module, name):
        # __main__ì—ì„œ ì €ìž¥ëœ í´ëž˜ìŠ¤ë“¤ì„ í˜„ìž¬ ëª¨ë“ˆì—ì„œ ì°¾ê¸°
        if module == '__main__':
            if name == 'DeepSupportClassifier':
                return DeepSupportClassifier
            elif name == 'AppleSiliconFeatureExtractor':
                return AppleSiliconFeatureExtractor
            elif name == 'EnhancedDeepLearningModel':
                return EnhancedDeepLearningModel
            elif name == 'EnhancedFeatureExtractor':
                return EnhancedFeatureExtractor
            elif name == 'ImprovedStartupClassifier':
                return ImprovedStartupClassifier
            elif name == 'PowerfulStartupClassifier':
                return PowerfulStartupClassifier
        
        # ê¸°ë³¸ ë™ìž‘
        return super().find_class(module, name)

# ============================================
# ì½”ëž©ì—ì„œ ì‚¬ìš©í•œ ëª¨ë¸ í´ëž˜ìŠ¤ ì •ì˜ (pickle ë¡œë”©ì„ ìœ„í•´ í•„ìš”)
# ============================================

class DeepSupportClassifier(nn.Module):
    """ê³ ì„±ëŠ¥ ë‹¤ì¸µ ì‹ ê²½ë§ ë¶„ë¥˜ê¸° (8ì¸µ Deep Network)"""
    
    def __init__(self, input_dim, hidden_dims=[1024, 512, 256, 128, 64, 32], dropout_rate=0.3):
        super(DeepSupportClassifier, self).__init__()
        
        # 8ì¸µ ê¹Šì€ ì‹ ê²½ë§ êµ¬ì¡° ì„¤ê³„
        layers = []
        prev_dim = input_dim
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ (ìž…ë ¥ì¸µ â†’ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ)
        layers.append(nn.Linear(prev_dim, hidden_dims[0]))
        layers.append(nn.BatchNorm1d(hidden_dims[0]))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout_rate))
        prev_dim = hidden_dims[0]
        
        # ì¤‘ê°„ ì€ë‹‰ì¸µë“¤ (2~7ì¸µ)
        for hidden_dim in hidden_dims[1:]:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        
        # ì¶œë ¥ì¸µ (8ë²ˆì§¸ ì¸µ)
        layers.append(nn.Linear(prev_dim, 2))  # ì´ì§„ ë¶„ë¥˜
        
        self.network = nn.Sequential(*layers)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Xavier Uniform)
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        return self.network(x)

class AppleSiliconFeatureExtractor:
    """Apple Silicon í˜¸í™˜ íŠ¹ì„± ì¶”ì¶œê¸°"""
    
    def __init__(self, device='cpu'):
        self.device = device
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ðŸ”§ íŠ¹ì„± ì¶”ì¶œê¸° ì´ˆê¸°í™”: {device}")
        
        # TF-IDF ë²¡í„°ë¼ì´ì €
        self.tfidf = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            stop_words=None,
            lowercase=True
        )
        self.tfidf_fitted = False
        
        # Sentence Transformer (Apple Silicon í˜¸í™˜)
        try:
            self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            if str(device) != 'cpu':
                self.sentence_model = self.sentence_model.to(device)
            self.logger.info("âœ… Sentence Transformer ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âš ï¸ Sentence Transformer ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.sentence_model = None
        
        # ì§€ì›ì‚¬ì—… ê´€ë ¨ í‚¤ì›Œë“œ
        self.support_keywords = [
            'ì§€ì›', 'ì‚¬ì—…', 'ìœ¡ì„±', 'ê°œë°œ', 'ì°½ì—…', 'í˜ì‹ ', 'ì—°êµ¬', 'ê¸°ìˆ ', 'íˆ¬ìž', 'ìœµìž',
            'ë³´ì¡°', 'í™œì„±í™”', 'í”„ë¡œê·¸ëž¨', 'r&d', 'ì—°êµ¬ê°œë°œ', 'ê¸°ìˆ ê°œë°œ', 'ì‚¬ì—…í™”', 'ìŠ¤íƒ€íŠ¸ì—…',
            'ë²¤ì²˜', 'ì¤‘ì†Œê¸°ì—…', 'ì†Œìƒê³µì¸', 'ì²­ë…„', 'ì—¬ì„±ê¸°ì—…', 'ì‚¬íšŒì ê¸°ì—…'
        ]
        
    def fit(self, texts):
        """íŠ¹ì„± ì¶”ì¶œê¸° í›ˆë ¨ (í›ˆë ¨ ë°ì´í„°ë¡œ TF-IDF í•™ìŠµ)"""
        self.logger.info("ðŸ”§ íŠ¹ì„± ì¶”ì¶œê¸° í›ˆë ¨ ì¤‘...")
        self.tfidf.fit(texts)
        self.tfidf_fitted = True
        self.logger.info("âœ… TF-IDF í›ˆë ¨ ì™„ë£Œ")
    
    def extract_manual_features(self, texts):
        """ìˆ˜ë™ íŠ¹ì„± ì¶”ì¶œ"""
        features = []
        
        for text in texts:
            text_lower = text.lower()
            
            feature_dict = {
                'length': len(text),
                'word_count': len(text.split()),
                'support_keyword_count': sum(1 for keyword in self.support_keywords if keyword in text_lower),
                'has_support_word': int('ì§€ì›' in text_lower),
                'has_business_word': int('ì‚¬ì—…' in text_lower),
                'has_startup_word': int(any(word in text_lower for word in ['ì°½ì—…', 'ìŠ¤íƒ€íŠ¸ì—…'])),
                'has_tech_word': int(any(word in text_lower for word in ['ê¸°ìˆ ', 'ê°œë°œ', 'ì—°êµ¬', 'r&d'])),
                'has_funding_word': int(any(word in text_lower for word in ['íˆ¬ìž', 'ìœµìž', 'ë³´ì¡°ê¸ˆ'])),
                'korean_ratio': len(re.findall(r'[ê°€-íž£]', text)) / max(len(text), 1),
                'number_count': len(re.findall(r'\d+', text))
            }
            
            features.append(list(feature_dict.values()))
        
        return np.array(features)
    
    def extract_features(self, texts, is_training=False):
        """ì „ì²´ íŠ¹ì„± ì¶”ì¶œ (Apple Silicon í˜¸í™˜)"""
        # TF-IDF íŠ¹ì„±
        if is_training or not self.tfidf_fitted:
            # í›ˆë ¨ ì‹œì—ë§Œ fit_transform ì‚¬ìš©
            tfidf_features = self.tfidf.fit_transform(texts).toarray()
            self.tfidf_fitted = True
        else:
            # ì˜ˆì¸¡ ì‹œì—ëŠ” transformë§Œ ì‚¬ìš©
            tfidf_features = self.tfidf.transform(texts).toarray()
        
        # Sentence embedding íŠ¹ì„± (Apple Silicon í˜¸í™˜)
        if self.sentence_model:
            try:
                # CPUë¡œ ë³€í™˜í•˜ì—¬ Apple Silicon í˜¸í™˜ì„± ë³´ìž¥
                sentence_features = []
                batch_size = 32
                
                for i in range(0, len(texts), batch_size):
                    batch_texts = texts[i:i+batch_size]
                    
                    # Apple Siliconì—ì„œëŠ” CPUë¡œ ì²˜ë¦¬
                    with torch.no_grad():
                        embeddings = self.sentence_model.encode(
                            batch_texts, 
                            convert_to_tensor=True,
                            device='cpu'  # Apple Silicon í˜¸í™˜ì„ ìœ„í•´ CPU ê°•ì œ ì‚¬ìš©
                        )
                        
                        if isinstance(embeddings, torch.Tensor):
                            embeddings = embeddings.cpu().numpy()
                        
                        sentence_features.extend(embeddings)
                
                sentence_features = np.array(sentence_features)
                
            except Exception as e:
                self.logger.error(f"âš ï¸ Sentence Embedding ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                sentence_features = np.zeros((len(texts), 384))  # ê¸°ë³¸ ì°¨ì›
        else:
            sentence_features = np.zeros((len(texts), 384))
        
        # ìˆ˜ë™ íŠ¹ì„±
        manual_features = self.extract_manual_features(texts)
        
        # íŠ¹ì„± ê²°í•©
        combined_features = np.hstack([
            tfidf_features,
            sentence_features, 
            manual_features
        ])
        
        return combined_features

# ============================================
# ë”¥ëŸ¬ë‹ ì—”ì§„ í´ëž˜ìŠ¤
# ============================================

class DeepLearningEngine:
    """ðŸ§  í”„ë¡œë•ì…˜ ë”¥ëŸ¬ë‹ ì—”ì§„"""
    
    def __init__(self, model_path='apple_silicon_production_model.pkl'):
        # ë¡œê¹… ì„¤ì • (ë¨¼ì € í•´ì•¼ í•¨)
        self.logger = logging.getLogger(__name__)
        
        # Apple Silicon ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._setup_device()
        
        # ëª¨ë¸ êµ¬ì„±ìš”ì†Œ
        self.model = None
        self.feature_extractor = None
        self.is_loaded = False
        self.model_info = {}
        
        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸ ë° ìˆ˜ì •
        from pathlib import Path
        model_file = Path(model_path)
        self.logger.info(f"ðŸ” ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸: {model_file.absolute()}")
        
        if not model_file.exists():
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì°¾ê¸°
            project_root = Path(__file__).parent.parent
            self.logger.info(f"ðŸ” í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root.absolute()}")
            
            alternative_path = project_root / model_path
            self.logger.info(f"ðŸ” ëŒ€ì•ˆ ê²½ë¡œ 1: {alternative_path.absolute()}")
            
            if alternative_path.exists():
                model_path = str(alternative_path)
                self.logger.info(f"ðŸ“ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
            else:
                # models ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
                models_path = project_root / 'models' / model_path
                self.logger.info(f"ðŸ” ëŒ€ì•ˆ ê²½ë¡œ 2 (models): {models_path.absolute()}")
                
                if models_path.exists():
                    model_path = str(models_path)
                    self.logger.info(f"ðŸ“ models ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
                else:
                    # models ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
                    models_dir = project_root / 'models'
                    if models_dir.exists():
                        self.logger.info(f"ðŸ“‚ models ë””ë ‰í† ë¦¬ ë‚´ìš©: {list(models_dir.iterdir())}")
                    self.logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
        else:
            self.logger.info(f"âœ… ëª¨ë¸ íŒŒì¼ ì¡´ìž¬: {model_file.absolute()}")
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_model(model_path)
        
    def _setup_device(self):
        """Apple Silicon (MPS) ë° CPU í˜¸í™˜ ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if torch.backends.mps.is_available():
            device = torch.device('mps')
            device_name = "Apple Silicon (MPS)"
        elif torch.cuda.is_available():
            device = torch.device('cuda')
            device_name = "NVIDIA GPU (CUDA)"
        else:
            device = torch.device('cpu')
            device_name = "CPU"
        
        self.logger.info(f"ðŸ–¥ï¸ ë”¥ëŸ¬ë‹ ì—”ì§„ ë””ë°”ì´ìŠ¤: {device_name}")
        return device
        
    def load_model(self, model_path):
        """ì½”ëž©ì—ì„œ í›ˆë ¨í•œ ëª¨ë¸ ë¡œë“œ"""
        self.logger.info(f"ðŸ“‚ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë”© ì‹œë„: {model_path}")
        
        try:
            # íŒŒì¼ ì¡´ìž¬ í™•ì¸
            from pathlib import Path
            if not Path(model_path).exists():
                raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            
            self.logger.info(f"âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸ë¨: {model_path}")
            
            # Apple Silicon í˜¸í™˜ì„ ìœ„í•œ íŠ¹ìˆ˜ ë¡œë”©
            original_load = torch.load
            torch.load = lambda *args, **kwargs: original_load(*args, **kwargs, map_location='cpu') if 'map_location' not in kwargs else original_load(*args, **kwargs)
            
            with open(model_path, 'rb') as f:
                unpickler = CustomUnpickler(f)
                
                # torch.load ëŒ€ì‹  unpickler ì‚¬ìš©
                model_data = unpickler.load()
            
            # ì›ëž˜ torch.load ë³µì›
            torch.load = original_load
            
            self.logger.info("âœ… ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì„±ê³µ")
            
            # ëª¨ë¸ íƒ€ìž… í™•ì¸
            if isinstance(model_data, dict):
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ëª¨ë¸ ë°ì´í„°
                self.model_info = {
                    'model_type': model_data.get('model_type', 'Unknown'),
                    'created_at': model_data.get('created_at', 'Unknown'),
                    'training_accuracy': model_data.get('training_accuracy', 0.0),
                    'version': model_data.get('version', '1.0')
                }
            elif hasattr(model_data, '__class__'):
                # EnhancedDeepLearningModel ê°™ì€ ê°ì²´
                self.model_info = {
                    'model_type': model_data.__class__.__name__,
                    'created_at': 'Unknown',
                    'training_accuracy': getattr(model_data, 'accuracy', 0.0),
                    'version': getattr(model_data, 'version', '1.0')
                }
                # ê°ì²´ë¥¼ ì§ì ‘ ì‚¬ìš©
                self.model = model_data
                self.model_loaded = True
                if hasattr(model_data, 'feature_extractor'):
                    self.feature_extractor = model_data.feature_extractor
                self.logger.info(f"âœ… {self.model_info['model_type']} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                return
            
            self.logger.info(f"âœ… ëª¨ë¸ íƒ€ìž…: {self.model_info['model_type']}")
            self.logger.info(f"âœ… í›ˆë ¨ ì •í™•ë„: {self.model_info['training_accuracy']:.4f}")
            
            # íŠ¹ì„± ì¶”ì¶œê¸° ë³µì›
            self.feature_extractor = model_data['feature_extractor']
            self.logger.info("âœ… íŠ¹ì„± ì¶”ì¶œê¸° ë¡œë“œ ì™„ë£Œ")
            
            # ë”¥ëŸ¬ë‹ ëª¨ë¸ ë³µì›
            if 'model_state_dict' in model_data:
                # ëª¨ë¸ êµ¬ì¡° ì •ë³´ì—ì„œ input_dim ì¶”ì¶œ
                model_structure = model_data['model_structure']
                input_dim = model_structure.network[0].in_features
                
                # ëª¨ë¸ ìž¬ìƒì„±
                self.model = DeepSupportClassifier(input_dim=input_dim)
                
                # Apple Silicon í˜¸í™˜ì„ ìœ„í•œ state_dict ë¡œë”©
                state_dict = model_data['model_state_dict']
                self.model.load_state_dict(state_dict)
                
                self.model = self.model.to(self.device)
                self.model.eval()
                
                self.logger.info(f"âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ìž…ë ¥ ì°¨ì›: {input_dim})")
            else:
                raise ValueError("ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            self.is_loaded = True
            self.logger.info("ðŸŽ‰ ê°•í™”í•™ìŠµ ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ!")
            
        except Exception as e:
            self.logger.error(f"âŒ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë”ë¯¸ ëª¨ë¸ë¡œ í´ë°±
            self._setup_fallback_model()
            
    def _setup_fallback_model(self):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª¨ë¸ ì„¤ì •"""
        self.logger.warning("âš ï¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
        self.is_loaded = False
        self.model_info = {
            'model_type': 'Fallback',
            'training_accuracy': 0.85,
            'status': 'fallback'
        }
        
    def predict(self, texts) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì˜ˆì¸¡"""
        if not self.is_loaded:
            # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ë¶„ë¥˜
            return self._fallback_predict(texts)
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì¸¡
        try:
            features = self.feature_extractor.extract_features(texts, is_training=False)
            features_tensor = torch.FloatTensor(features).to(self.device)
            
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(features_tensor)
                _, predicted = torch.max(outputs.data, 1)
                
            return predicted.cpu().numpy()
            
        except Exception as e:
            self.logger.error(f"âŒ ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return self._fallback_predict(texts)
    
    def predict_proba(self, texts) -> np.ndarray:
        """í™•ë¥  ì˜ˆì¸¡"""
        if not self.is_loaded:
            # í´ë°±: ê°„ë‹¨í•œ í™•ë¥  ê³„ì‚°
            predictions = self._fallback_predict(texts)
            probabilities = np.array([[0.9, 0.1] if p == 0 else [0.1, 0.9] for p in predictions])
            return probabilities
        
        try:
            features = self.feature_extractor.extract_features(texts, is_training=False)
            features_tensor = torch.FloatTensor(features).to(self.device)
            
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(features_tensor)
                probabilities = F.softmax(outputs, dim=1)
                
            return probabilities.cpu().numpy()
            
        except Exception as e:
            self.logger.error(f"âŒ ë”¥ëŸ¬ë‹ í™•ë¥  ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            predictions = self._fallback_predict(texts)
            probabilities = np.array([[0.9, 0.1] if p == 0 else [0.1, 0.9] for p in predictions])
            return probabilities
    
    def _fallback_predict(self, texts) -> np.ndarray:
        """í´ë°± ì˜ˆì¸¡ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        support_keywords = [
            'ì§€ì›', 'ì‚¬ì—…', 'ìœ¡ì„±', 'ê°œë°œ', 'ì°½ì—…', 'í˜ì‹ ', 'ì—°êµ¬', 'ê¸°ìˆ ', 'íˆ¬ìž', 'ìœµìž',
            'ë³´ì¡°', 'í™œì„±í™”', 'í”„ë¡œê·¸ëž¨', 'r&d', 'ì—°êµ¬ê°œë°œ', 'ê¸°ìˆ ê°œë°œ', 'ì‚¬ì—…í™”', 'ìŠ¤íƒ€íŠ¸ì—…',
            'ë²¤ì²˜', 'ì¤‘ì†Œê¸°ì—…', 'ì†Œìƒê³µì¸', 'ì²­ë…„', 'ì—¬ì„±ê¸°ì—…', 'ì‚¬íšŒì ê¸°ì—…'
        ]
        
        predictions = []
        for text in texts:
            text_lower = text.lower()
            keyword_count = sum(1 for keyword in support_keywords if keyword in text_lower)
            # 3ê°œ ì´ìƒ í‚¤ì›Œë“œê°€ ìžˆìœ¼ë©´ ì§€ì›ì‚¬ì—…ìœ¼ë¡œ ë¶„ë¥˜
            is_support = 1 if keyword_count >= 3 else 0
            predictions.append(is_support)
            
        return np.array(predictions)
    
    def calculate_score(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì§€ì›ì‚¬ì—… ì ìˆ˜ ê³„ì‚° (0~100)"""
        try:
            # EnhancedDeepLearningModelì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
            if hasattr(self.model, 'calculate_ai_score'):
                return self.model.calculate_ai_score(text)
            # ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©
            else:
                probabilities = self.predict_proba([text])
                # ì§€ì›ì‚¬ì—…ì¼ í™•ë¥  * 100
                score = probabilities[0][1] * 100
                return float(score)
        except Exception as e:
            self.logger.error(f"âŒ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 50.0  # ê¸°ë³¸ê°’
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            **self.model_info,
            'device': str(self.device),
            'is_loaded': self.is_loaded,
            'available': self.is_loaded
        }
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return {
            'accuracy': self.model_info.get('training_accuracy', 0.0),
            'model_type': self.model_info.get('model_type', 'Unknown'),
            'status': 'active' if self.is_loaded else 'fallback',
            'layers': 8 if self.is_loaded else 1,
            'parameters': 'High' if self.is_loaded else 'Low'
        }

# ì „ì—­ ë”¥ëŸ¬ë‹ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
_deep_learning_engine = None

def get_deep_learning_engine(model_path=None) -> DeepLearningEngine:
    """ë”¥ëŸ¬ë‹ ì—”ì§„ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _deep_learning_engine
    if _deep_learning_engine is None:
        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì • (models ë””ë ‰í† ë¦¬)
        if model_path is None:
            from pathlib import Path
            project_root = Path(__file__).parent.parent
            model_path = str(project_root / 'models' / 'apple_silicon_production_model.pkl')
        _deep_learning_engine = DeepLearningEngine(model_path)
    return _deep_learning_engine

def reload_deep_learning_engine(model_path=None) -> DeepLearningEngine:
    """ë”¥ëŸ¬ë‹ ì—”ì§„ ìž¬ë¡œë“œ"""
    global _deep_learning_engine
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì • (models ë””ë ‰í† ë¦¬)
    if model_path is None:
        from pathlib import Path
        project_root = Path(__file__).parent.parent
        model_path = str(project_root / 'models' / 'apple_silicon_production_model.pkl')
    _deep_learning_engine = DeepLearningEngine(model_path)
    return _deep_learning_engine


# ============================================
# ê°•í™”í•™ìŠµ ëª¨ë¸ í´ëž˜ìŠ¤ë“¤ (ì½”ëž© í˜¸í™˜ì„±)
# ============================================

class EnhancedFeatureExtractor:
    """í–¥ìƒëœ íŠ¹ì„± ì¶”ì¶œê¸°"""
    
    def __init__(self, sentence_model_name='jhgan/ko-sroberta-multitask'):
        self.sentence_model = SentenceTransformer(sentence_model_name)
        
    def extract_features(self, texts, is_training=True):
        """í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        if isinstance(texts, str):
            texts = [texts]
        
        # Sentence embeddings
        embeddings = self.sentence_model.encode(texts)
        
        # í‚¤ì›Œë“œ íŠ¹ì„± ì¶”ê°€
        keyword_features = []
        for text in texts:
            text_lower = text.lower()
            features = {
                'support_keywords': sum([
                    'tips' in text_lower,
                    'ì°½ì—…' in text_lower,
                    'ì§€ì›' in text_lower,
                    'ì‚¬ì—…' in text_lower,
                    'k-ìŠ¤íƒ€íŠ¸ì—…' in text_lower,
                    'ì°½ì—…ì§„í¥ì›' in text_lower,
                ]) * 10,
                'spam_keywords': sum([
                    'ê´‘ê³ ' in text_lower,
                    'í™ë³´' in text_lower,
                    'ìº íŽ˜ì¸' in text_lower,
                    'ìˆ˜ë£Œì‹' in text_lower,
                    'ì´ë²¤íŠ¸' in text_lower,
                ]) * (-15),
                'context_features': sum([
                    bool(re.search(r'\d+ì–µì›|\d+ì²œë§Œì›|\d+ë°±ë§Œì›', text)),
                    bool(re.search(r'ì‹ ì²­ê¸°ê°„|ë§ˆê°ì¼|ì ‘ìˆ˜ê¸°ê°„', text)),
                    bool(re.search(r'ì§€ì›ëŒ€ìƒ|ì‹ ì²­ìžê²©', text)),
                    bool(re.search(r'ì •ë¶€|ê³µê³µê¸°ê´€|ì§„í¥ì›', text)),
                ]) * 5
            }
            keyword_features.append(sum(features.values()))
        
        # íŠ¹ì„± ê²°í•©
        keyword_features = np.array(keyword_features).reshape(-1, 1)
        combined_features = np.hstack([embeddings, keyword_features])
        
        return combined_features


class ImprovedStartupClassifier(nn.Module):
    """ê°œì„ ëœ ìŠ¤íƒ€íŠ¸ì—… ë¶„ë¥˜ê¸°"""
    
    def __init__(self, input_dim=769, hidden_dim=512):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, 1)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x


class PowerfulStartupClassifier(nn.Module):
    """ê°•ë ¥í•œ ìŠ¤íƒ€íŠ¸ì—… ë¶„ë¥˜ê¸° (A100 ìµœì í™”)"""
    
    def __init__(self, input_dim=769, hidden_dim=1024):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 128)
        self.fc5 = nn.Linear(128, 1)
        self.dropout = nn.Dropout(0.3)
        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)
        self.batch_norm2 = nn.BatchNorm1d(512)
        self.batch_norm3 = nn.BatchNorm1d(256)
        self.batch_norm4 = nn.BatchNorm1d(128)
        
    def forward(self, x):
        x = F.relu(self.batch_norm1(self.fc1(x)))
        x = self.dropout(x)
        x = F.relu(self.batch_norm2(self.fc2(x)))
        x = self.dropout(x)
        x = F.relu(self.batch_norm3(self.fc3(x)))
        x = self.dropout(x)
        x = F.relu(self.batch_norm4(self.fc4(x)))
        x = self.fc5(x)
        return x


class EnhancedDeepLearningModel:
    """í–¥ìƒëœ ë”¥ëŸ¬ë‹ ëª¨ë¸ ëž˜í¼"""
    
    def __init__(self, feature_extractor, classifier, device='cuda'):
        self.feature_extractor = feature_extractor
        self.classifier = classifier
        
        # ë””ë°”ì´ìŠ¤ ìžë™ ê°ì§€
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        elif torch.backends.mps.is_available():
            self.device = torch.device('mps')
        else:
            self.device = torch.device('cpu')
        
        # ëª¨ë¸ì„ CPUë¡œ ìœ ì§€ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        self.classifier = self.classifier.cpu()
        
        # ë™ì  ìž„ê³„ê°’ (ì´ˆê¸°ê°’ì„ ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
        self.threshold = 0.65  # 55ì  ëŒ€ì‹  65ì ë¶€í„°
        self.adaptive_threshold = True
        self.threshold_min = 0.55
        self.threshold_max = 0.85
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ (ë”¥ëŸ¬ë‹, í‚¤ì›Œë“œ, íŒ¨í„´)
        self.model_weights = {
            'deep_learning': 0.6,
            'keyword_score': 0.3,
            'pattern_score': 0.1
        }
        
    def predict(self, texts):
        """í…ìŠ¤íŠ¸ ì˜ˆì¸¡"""
        if isinstance(texts, str):
            texts = [texts]
        
        features = self.feature_extractor.extract_features(texts, is_training=False)
        features_tensor = torch.FloatTensor(features).to(self.device)
        
        self.classifier = self.classifier.to(self.device)
        self.classifier.eval()
        
        with torch.no_grad():
            outputs = self.classifier(features_tensor)
            probabilities = torch.sigmoid(outputs).cpu().numpy()
        
        self.classifier = self.classifier.cpu()
        
        return (probabilities > self.threshold).astype(int).flatten()
    
    def calculate_ai_score(self, text):
        """AI ì ìˆ˜ ê³„ì‚° (ì°½ì—…/ìŠ¤íƒ€íŠ¸ì—… ì§€ì›ì‚¬ì—… íŠ¹í™”)"""
        import re
        
        # 1. ë”¥ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì¸¡
        features = self.feature_extractor.extract_features([text], is_training=False)
        features_tensor = torch.FloatTensor(features).to(self.device)
        
        self.classifier = self.classifier.to(self.device)
        self.classifier.eval()
        
        with torch.no_grad():
            output = self.classifier(features_tensor)
            probability = torch.sigmoid(output).cpu().item()
        
        self.classifier = self.classifier.cpu()
        
        # 2. ì°½ì—…/ìŠ¤íƒ€íŠ¸ì—… ì§€ì›ì‚¬ì—… í•„í„°ë§
        text_lower = text.lower()
        title_part = text.split('\n')[0] if '\n' in text else text[:100]
        title_lower = title_part.lower()
        
        # === ë°°ì œ í‚¤ì›Œë“œ ì²´í¬ (ë¬´ê´€í•œ ì§€ì› ì œì™¸) ===
        exclude_keywords = {
            # ê°œì¸ ì§€ì› (ì°½ì—…ê³¼ ë¬´ê´€)
            'ì–‘ìœ¡ë¹„': -50, 'ìœ¡ì•„': -50, 'ì¶œì‚°': -50, 'ìž„ì‹ ': -50,
            'ìž¥í•™ê¸ˆ': -50, 'í•™ìžê¸ˆ': -50, 'ìƒí™œë¹„': -50, 'ì£¼ê±°ë¹„': -50,
            'ì˜ë£Œë¹„': -50, 'ì¹˜ë£Œë¹„': -50, 'ê°„ë³‘': -50, 'ë³µì§€': -40,
            
            # êµ¬ì§/ì±„ìš© ê´€ë ¨
            'ìž…ì‚¬ì§€ì›': -50, 'ì±„ìš©': -40, 'êµ¬ì¸': -40, 'êµ¬ì§': -40,
            'ì¸í„´': -30, 'ì§ì›ëª¨ì§‘': -40, 'ì¸ìž¬ì±„ìš©': -40,
            
            # êµìœ¡/ìˆ˜ë£Œ ê´€ë ¨ (ë‹¨ìˆœ êµìœ¡)
            'ìˆ˜ë£Œì‹': -50, 'ì¡¸ì—…ì‹': -50, 'ìž…í•™': -40, 'ê°œê°•': -40,
            'ìžê²©ì¦': -30, 'ì‹œí—˜': -30, 'í•©ê²©': -30,
            
            # ì´ë²¤íŠ¸/í–‰ì‚¬
            'ì¶•ì œ': -50, 'ê³µì—°': -50, 'ì „ì‹œíšŒ': -40, 'ë°•ëžŒíšŒ': -20,
            'ì„¸ë¯¸ë‚˜': -20, 'í¬ëŸ¼': -20, 'ì»¨í¼ëŸ°ìŠ¤': -10,
            
            # ê´‘ê³ /í™ë³´
            'ê´‘ê³ ': -50, 'í™ë³´': -40, 'ë§ˆì¼€íŒ…': -30, 'ì´ë²¤íŠ¸': -30,
            'í• ì¸': -50, 'ì¿ í°': -50, 'í”„ë¡œëª¨ì…˜': -40,
            
            # ê¸°íƒ€ ë¬´ê´€í•œ ë¶„ì•¼
            'ë¶€ë™ì‚°': -50, 'ì•„íŒŒíŠ¸': -50, 'ë¶„ì–‘': -50,
            'ì¹´íŽ˜': -50, 'ë§›ì§‘': -50, 'ìŒì‹ì ': -50,
            'ê´€ê´‘': -40, 'ì—¬í–‰': -40, 'ìˆ™ë°•': -40,
            'ì¢…êµ': -50, 'êµíšŒ': -50, 'ì ˆ': -50,
            'ìŠ¤í¬ì¸ ': -40, 'ìš´ë™': -40, 'í—¬ìŠ¤': -40
        }
        
        # === í•„ìˆ˜ í‚¤ì›Œë“œ ì²´í¬ (ì°½ì—… ì§€ì›ì‚¬ì—… í™•ì¸) ===
        essential_contexts = {
            # ìžê¸ˆ ì§€ì› ë§¥ë½
            'funding': any([
                'íˆ¬ìž' in text_lower and ('ìœ ì¹˜' in text_lower or 'ì§€ì›' in text_lower),
                'ìžê¸ˆ' in text_lower and 'ì§€ì›' in text_lower,
                'ìœµìž' in text_lower and ('ì§€ì›' in text_lower or 'ì‚¬ì—…' in text_lower),
                'ë³´ì¡°ê¸ˆ' in text_lower,
                'ì§€ì›ê¸ˆ' in text_lower,
                bool(re.search(r'\d+ì–µì›.*ì§€ì›|\d+ì²œë§Œì›.*ì§€ì›|\d+ë°±ë§Œì›.*ì§€ì›', text_lower))
            ]),
            
            # ì°½ì—…/ìŠ¤íƒ€íŠ¸ì—… ë§¥ë½
            'startup': any([
                'ì°½ì—…' in text_lower and ('ì§€ì›' in text_lower or 'ìœ¡ì„±' in text_lower or 'ë³´ìœ¡' in text_lower),
                'ìŠ¤íƒ€íŠ¸ì—…' in text_lower and ('ì§€ì›' in text_lower or 'ìœ¡ì„±' in text_lower),
                'ë²¤ì²˜' in text_lower and ('ì§€ì›' in text_lower or 'ìœ¡ì„±' in text_lower),
                'ì˜ˆë¹„ì°½ì—…' in text_lower,
                'ì´ˆê¸°ì°½ì—…' in text_lower,
                'ê¸°ìˆ ì°½ì—…' in text_lower
            ]),
            
            # ê¸°ì—… ì§€ì› ë§¥ë½
            'business': any([
                'ì¤‘ì†Œê¸°ì—…' in text_lower and 'ì§€ì›' in text_lower,
                'ì†Œìƒê³µì¸' in text_lower and 'ì§€ì›' in text_lower,
                'ê¸°ì—…' in text_lower and ('ìœ¡ì„±' in text_lower or 'ì§€ì›ì‚¬ì—…' in text_lower),
                'r&d' in text_lower and 'ì§€ì›' in text_lower,
                'ê¸°ìˆ ê°œë°œ' in text_lower and 'ì§€ì›' in text_lower
            ]),
            
            # ê³µì‹ ê¸°ê´€ ë§¥ë½
            'official': any([
                'tips' in text_lower,
                'ì°½ì—…ì§„í¥ì›' in text_lower,
                'k-ìŠ¤íƒ€íŠ¸ì—…' in text_lower,
                'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€' in text_lower,
                'ì¤‘ê¸°ë¶€' in text_lower,
                'ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€' in text_lower,
                'ê³¼ê¸°ì •í†µë¶€' in text_lower,
                'ì‚°ì—…í†µìƒìžì›ë¶€' in text_lower,
                'ì‚°ì—…ë¶€' in text_lower
            ])
        }
        
        # === ì ìˆ˜ ê³„ì‚° ë¡œì§ ===
        keyword_score = 0
        
        # 1. ë°°ì œ í‚¤ì›Œë“œ ì²´í¬ (ì œëª©ì— ìžˆìœ¼ë©´ ë” ê°•í•œ íŽ˜ë„í‹°)
        for keyword, penalty in exclude_keywords.items():
            if keyword in title_lower:
                keyword_score += penalty * 2  # ì œëª©ì— ìžˆìœ¼ë©´ 2ë°° ê°ì 
                logging.info(f"ðŸš« ë°°ì œ í‚¤ì›Œë“œ ë°œê²¬ (ì œëª©): '{keyword}' â†’ {penalty * 2}ì ")
            elif keyword in text_lower:
                keyword_score += penalty
                logging.info(f"ðŸš« ë°°ì œ í‚¤ì›Œë“œ ë°œê²¬: '{keyword}' â†’ {penalty}ì ")
        
        # 2. í•„ìˆ˜ ë§¥ë½ ì²´í¬
        context_count = sum(1 for context in essential_contexts.values() if context)
        
        # í•„ìˆ˜ ë§¥ë½ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ í° ê°ì 
        if context_count == 0:
            keyword_score -= 30
            logging.info("âš ï¸ ì°½ì—…/ê¸°ì—… ì§€ì› ë§¥ë½ ì—†ìŒ â†’ -30ì ")
        else:
            # ë§¥ë½ì´ ìžˆìœ¼ë©´ ê°€ì 
            keyword_score += context_count * 15
            logging.info(f"âœ… ì§€ì›ì‚¬ì—… ë§¥ë½ {context_count}ê°œ ë°œê²¬ â†’ +{context_count * 15}ì ")
        
        # 3. ê³µì‹ ê¸°ê´€ ì¶”ê°€ ê°€ì 
        if essential_contexts['official']:
            keyword_score += 25
            logging.info("ðŸ¢ ê³µì‹ ê¸°ê´€ ì§€ì›ì‚¬ì—… â†’ +25ì ")
        
        # 4. ìƒì„¸ ì •ë³´ ì²´í¬
        detail_score = 0
        
        # ì§€ì› ê¸ˆì•¡ ëª…ì‹œ
        if re.search(r'\d+ì–µì›|\d+ì²œë§Œì›|\d+ë°±ë§Œì›', text):
            detail_score += 15
            logging.info("ðŸ’° ì§€ì› ê¸ˆì•¡ ì •ë³´ â†’ +15ì ")
        
        # ì‹ ì²­ ê¸°ê°„ ëª…ì‹œ
        if re.search(r'ì‹ ì²­ê¸°ê°„|ë§ˆê°ì¼|ì ‘ìˆ˜ê¸°ê°„|ëª¨ì§‘ê¸°ê°„|ì‹ ì²­.*~.*\d+ì›”', text):
            detail_score += 10
            logging.info("ðŸ“… ì‹ ì²­ ê¸°ê°„ ì •ë³´ â†’ +10ì ")
        
        # ì§€ì› ëŒ€ìƒ ëª…ì‹œ
        if re.search(r'ì§€ì›ëŒ€ìƒ|ì‹ ì²­ìžê²©|ëŒ€ìƒê¸°ì—…|ì§€ì›ìžê²©', text):
            detail_score += 10
            logging.info("ðŸŽ¯ ì§€ì› ëŒ€ìƒ ì •ë³´ â†’ +10ì ")
        
        # 5. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        final_score = (
            probability * 100 * self.model_weights['deep_learning'] +
            keyword_score * self.model_weights['keyword_score'] +
            detail_score * self.model_weights['pattern_score']
        )
        
        # ì ìˆ˜ ë²”ìœ„ ì œí•œ (0-100)
        final_score = max(0, min(100, final_score))
        
        # ë””ë²„ê¹… ì •ë³´
        logging.info(f"ðŸ“Š AI ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {final_score:.1f}ì  (DL: {probability*100:.1f}, KW: {keyword_score}, DT: {detail_score})")
        
        return final_score
    
    def update_threshold(self, feedback_results):
        """í”¼ë“œë°± ê¸°ë°˜ ìž„ê³„ê°’ ì—…ë°ì´íŠ¸"""
        if not self.adaptive_threshold:
            return
        
        # ì„±ê³¼ ê³„ì‚°
        tp = feedback_results.get('true_positive', 0)
        fp = feedback_results.get('false_positive', 0)
        fn = feedback_results.get('false_negative', 0)
        
        if tp + fp > 0:
            precision = tp / (tp + fp)
        else:
            precision = 0
        
        if tp + fn > 0:
            recall = tp / (tp + fn)
        else:
            recall = 0
        
        # ìž„ê³„ê°’ ì¡°ì •
        if precision < 0.7:  # ì •ë°€ë„ê°€ ë‚®ìœ¼ë©´
            self.threshold = min(self.threshold + 0.05, self.threshold_max)
        elif recall < 0.7:  # ìž¬í˜„ìœ¨ì´ ë‚®ìœ¼ë©´
            self.threshold = max(self.threshold - 0.05, self.threshold_min)
        
        logging.info(f"ðŸŽ¯ ìž„ê³„ê°’ ì—…ë°ì´íŠ¸: {self.threshold:.3f} (ì •ë°€ë„: {precision:.3f}, ìž¬í˜„ìœ¨: {recall:.3f})") 