#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ ìŠ¤íƒ€íŠ¸ì—… ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ - ê°•í™”í•™ìŠµ ìµœì í™”
====================================================

ì½”ë©ì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ê°•í™”í•™ìŠµìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”

ì£¼ìš” ìµœì í™” ëª©í‘œ:
1. ì„ê³„ê°’ ë™ì  ì¡°ì •
2. ëª¨ë¸ ê°€ì¤‘ì¹˜ ìµœì í™”  
3. ëŠ¥ë™í•™ìŠµ ì „ëµ
"""

import os
import json
import pickle
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
from datetime import datetime
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
from .logger import get_logger

# ê°•í™”í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ìˆ˜ì •ëœ import)
try:
    import gymnasium as gym
    from gymnasium import spaces
    GYM_AVAILABLE = True
except ImportError:
    try:
        import gym
        from gym import spaces
        GYM_AVAILABLE = True
    except ImportError:
        GYM_AVAILABLE = False

try:
    from stable_baselines3 import PPO, A2C, DQN
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import VecNormalize
    SB3_AVAILABLE = True
except ImportError:
    SB3_AVAILABLE = False

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split

# ë¡œì»¬ ëª¨ë“ˆ import
from .config import Config
from .database import get_database_manager


if GYM_AVAILABLE:
    BaseEnv = gym.Env
else:
    # gymì´ ì—†ëŠ” ê²½ìš° ë”ë¯¸ í´ë˜ìŠ¤
    class BaseEnv:
        def __init__(self): pass
        def reset(self, **kwargs): return np.array([0.0]), {}
        def step(self, action): return np.array([0.0]), 0.0, True, False, {}

class StartupClassifierEnv(BaseEnv):
    """ìŠ¤íƒ€íŠ¸ì—… ë¶„ë¥˜ê¸° ê°•í™”í•™ìŠµ í™˜ê²½"""
    
    def __init__(self, model, test_texts, test_labels, mode='threshold'):
        super().__init__()
        
        self.model = model
        self.test_texts = test_texts
        self.test_labels = test_labels
        self.mode = mode  # 'threshold' or 'weights'
        
        # í˜„ì¬ ì—í”¼ì†Œë“œ ìƒíƒœ
        self.current_step = 0
        self.max_steps = 100
        
        if GYM_AVAILABLE:
            if mode == 'threshold':
                # ì„ê³„ê°’ ìµœì í™” ëª¨ë“œ (0.1 ~ 0.9)
                self.action_space = spaces.Box(low=0.1, high=0.9, shape=(1,), dtype=np.float32)
                self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)
                
            elif mode == 'weights':
                # ëª¨ë¸ ê°€ì¤‘ì¹˜ ìµœì í™” ëª¨ë“œ (7ê°œ ëª¨ë¸)
                num_models = len(self.model.model_weights) if hasattr(self.model, 'model_weights') else 3
                self.action_space = spaces.Box(low=0.0, high=1.0, shape=(num_models,), dtype=np.float32)
                self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        else:
            # gymì´ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ì„¤ì •
            self.action_space = None
            self.observation_space = None
        
        # ì„±ê³¼ ê¸°ë¡
        self.best_score = 0
        self.performance_history = []
        
    def reset(self, seed=None, options=None):
        """í™˜ê²½ ì´ˆê¸°í™” (Gymnasium í˜¸í™˜)"""
        super().reset(seed=seed)
        self.current_step = 0
        
        if self.mode == 'threshold':
            # í˜„ì¬ ì„ê³„ê°’, ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1
            obs = np.array([0.5, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)
        else:
            # í˜„ì¬ ê°€ì¤‘ì¹˜ë“¤ + ì„±ê³¼ ì§€í‘œë“¤
            weights = list(getattr(self.model, 'model_weights', {0: 1.0, 1: 1.0, 2: 1.0}).values())
            metrics = [0.0, 0.0, 0.0, 0.0]  # accuracy, precision, recall, f1
            obs = np.array(weights + metrics, dtype=np.float32)
        
        # Gymnasium í˜¸í™˜ì„ ìœ„í•´ (observation, info) íŠœí”Œ ë°˜í™˜
        info = {
            'step': self.current_step,
            'mode': self.mode,
            'best_score': self.best_score
        }
        return obs, info
    
    def step(self, action):
        """í•œ ìŠ¤í… ì‹¤í–‰"""
        self.current_step += 1
        
        # ì•¡ì…˜ ì ìš©
        if self.mode == 'threshold':
            new_threshold = float(action[0])
            if hasattr(self.model, 'threshold'):
                self.model.threshold = new_threshold
            
        elif self.mode == 'weights':
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            normalized_weights = action / np.sum(action)
            if hasattr(self.model, 'model_weights'):
                model_names = list(self.model.model_weights.keys())
                for i, name in enumerate(model_names):
                    if i < len(normalized_weights):
                        self.model.model_weights[name] = float(normalized_weights[i])
        
        # ì„±ê³¼ í‰ê°€
        try:
            if hasattr(self.model, 'predict'):
                predictions = self.model.predict(self.test_texts)
            else:
                # ê¸°ë³¸ ì˜ˆì¸¡ (ëª¨ë¸ì´ predict ë©”ì„œë“œê°€ ì—†ëŠ” ê²½ìš°)
                predictions = [1] * len(self.test_texts)
            
            accuracy = accuracy_score(self.test_labels, predictions)
            precision = precision_score(self.test_labels, predictions, zero_division=0)
            recall = recall_score(self.test_labels, predictions, zero_division=0)
            f1 = f1_score(self.test_labels, predictions, zero_division=0)
            
            # ë³µí•© ë³´ìƒ í•¨ìˆ˜
            reward = self._calculate_reward(accuracy, precision, recall, f1)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            if self.mode == 'threshold':
                threshold = getattr(self.model, 'threshold', 0.5)
                next_state = np.array([threshold, accuracy, precision, recall, f1], dtype=np.float32)
            else:
                weights = list(getattr(self.model, 'model_weights', {0: 1.0, 1: 1.0, 2: 1.0}).values())
                next_state = np.array(weights + [accuracy, precision, recall, f1], dtype=np.float32)
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´
            done = self.current_step >= self.max_steps
            truncated = False
            
            # ì„±ê³¼ ê¸°ë¡
            self.performance_history.append({
                'step': self.current_step,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'reward': reward
            })
            
            if accuracy > self.best_score:
                self.best_score = accuracy
            
        except Exception as e:
            self.logger.error(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            reward = -1.0
            next_state, _ = self.reset()
            done = True
            truncated = False
        
        return next_state, reward, done, truncated, {}
    
    def _calculate_reward(self, accuracy, precision, recall, f1):
        """ë³´ìƒ í•¨ìˆ˜ ê³„ì‚°"""
        # ì§€ì›ì‚¬ì—… íƒì§€ë¥¼ ìœ„í•´ ì¬í˜„ìœ¨ì— ë†’ì€ ê°€ì¤‘ì¹˜
        base_reward = 0.4 * accuracy + 0.2 * precision + 0.3 * recall + 0.1 * f1
        
        # ë³´ë„ˆìŠ¤: ë†’ì€ ì„±ê³¼ì— ëŒ€í•œ ì¶”ê°€ ë³´ìƒ
        if accuracy > 0.9:
            base_reward += 0.1
        if recall > 0.85:  # ì§€ì›ì‚¬ì—… ë†“ì¹˜ì§€ ì•Šê¸°
            base_reward += 0.1
        if precision > 0.8:  # ì˜¤íƒì§€ ìµœì†Œí™”
            base_reward += 0.05
            
        return base_reward


if GYM_AVAILABLE:
    BaseWrapper = gym.Wrapper
else:
    # gymì´ ì—†ëŠ” ê²½ìš° ë”ë¯¸ í´ë˜ìŠ¤
    class BaseWrapper:
        def __init__(self, env): self.env = env
        def reset(self, **kwargs): return np.array([0.0]), {}
        def step(self, action): return np.array([0.0]), 0.0, True, False, {}

class StableBaselinesEnvWrapper(BaseWrapper):
    """Stable-baselines3ì™€ Gymnasium í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    
    def __init__(self, env):
        super().__init__(env)
        
    def reset(self, **kwargs):
        """resetì—ì„œ observationë§Œ ë°˜í™˜ (stable-baselines3ìš©)"""
        obs, _ = self.env.reset(**kwargs)
        return obs
    
    def step(self, action):
        """stepì€ ê·¸ëŒ€ë¡œ ìœ ì§€"""
        return self.env.step(action)


class ReinforcementLearningOptimizer:
    """ê°•í™”í•™ìŠµ ê¸°ë°˜ ëª¨ë¸ ìµœì í™”"""
    
    def __init__(self, model_path: str = None, test_data_path: str = None, ai_engine=None):
        """ì´ˆê¸°í™”"""
        self.logger = get_logger(__name__)
        self.logger.info("ğŸš€ ê°•í™”í•™ìŠµ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”!")
        self.logger.info("="*50)
        
        # ì˜ì¡´ì„± ì²´í¬
        if not GYM_AVAILABLE:
            raise ImportError("gymnasium ë˜ëŠ” gymì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install gymnasium")
        if not SB3_AVAILABLE:
            raise ImportError("stable-baselines3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install stable-baselines3")
        if not OPTUNA_AVAILABLE:
            self.logger.warning("âš ï¸ optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
        if ai_engine:
            # AI ì—”ì§„ìœ¼ë¡œë¶€í„° ëª¨ë¸ ì‚¬ìš©
            self.model = ai_engine
            self.test_texts, self.test_labels = self._get_test_data_from_ai_engine(ai_engine)
            self.logger.info("âœ… AI ì—”ì§„ìœ¼ë¡œë¶€í„° ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        else:
            # íŒŒì¼ë¡œë¶€í„° ë¡œë“œ
            if model_path and os.path.exists(model_path):
                self.logger.info("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
                with open(model_path, 'rb') as f:
                    self.model = pickle.load(f)
                
                # ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸ ë° ë¡œë“œ
                metadata_path = model_path.replace('.pkl', '_metadata.json')
                if os.path.exists(metadata_path):
                    self.logger.info("ğŸ“‹ ëª¨ë¸ ë©”íƒ€ë°ì´í„° ë¡œë”© ì¤‘...")
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    # ë©”íƒ€ë°ì´í„° ì ìš©
                    if hasattr(self.model, 'threshold'):
                        self.model.threshold = metadata.get('threshold', 0.3)
                        self.logger.info(f"âœ… ì„ê³„ê°’ ì„¤ì •: {self.model.threshold}")
                    
                    if hasattr(self.model, 'model_weights') and 'model_weights' in metadata:
                        self.model.model_weights = metadata['model_weights']
                        self.logger.info(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì„¤ì •: {self.model.model_weights}")
                    
                    # ì„±ëŠ¥ ì •ë³´ ì¶œë ¥
                    if 'performance' in metadata:
                        perf = metadata['performance']
                        self.logger.info(f"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ - ì •í™•ë„: {perf.get('accuracy', 0):.1%}, F1: {perf.get('f1', 0):.3f}")
            else:
                self.logger.warning("âš ï¸ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.model = self._create_dummy_model()
            
            if test_data_path and os.path.exists(test_data_path):
                self.logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘...")
                with open(test_data_path, 'r', encoding='utf-8') as f:
                    test_data = json.load(f)
                self.test_texts = test_data['texts']
                self.test_labels = test_data['labels']
            else:
                self.logger.warning("âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.test_texts, self.test_labels = self._create_dummy_test_data()
        
        # ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (ë¬¸ì œ ì¼€ì´ìŠ¤ë“¤)
        additional_cases = [
            ('ë²¤ì²˜íˆ¬ì í€ë”© í”„ë¡œê·¸ë¨ ì°¸ê°€ì ëª¨ì§‘', 1),
            ('2024 K-ìŠ¤íƒ€íŠ¸ì—… ì„¼í„° ì…ì£¼ê¸°ì—… ëª¨ì§‘', 1),
            ('ì‹ ê¸°ìˆ  ìŠ¤íƒ€íŠ¸ì—… ìœ¡ì„±ì‚¬ì—… ê³µê³ ', 1),
            ('ì¼ë°˜ ìƒí’ˆ ê´‘ê³  ì•ˆë‚´', 0),
            ('ì¹´í˜ ì‹ ë©”ë‰´ ì¶œì‹œ ì´ë²¤íŠ¸', 0),
            ('ë¶€ë™ì‚° íˆ¬ì ìƒë‹´ ì„œë¹„ìŠ¤', 0),
        ]
        
        for text, label in additional_cases:
            self.test_texts.append(text)
            self.test_labels.append(label)
        
        self.logger.info(f"âœ… ì´ {len(self.test_texts)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        
        # ì´ˆê¸° ì„±ê³¼ ì¸¡ì •
        self.baseline_performance = self._evaluate_model()
        self.logger.info(f"ğŸ¯ ë² ì´ìŠ¤ë¼ì¸ ì„±ê³¼: {self.baseline_performance['accuracy']:.3f}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        self.db = get_database_manager()
        
    def _get_test_data_from_ai_engine(self, ai_engine) -> Tuple[List[str], List[int]]:
        """AI ì—”ì§„ìœ¼ë¡œë¶€í„° ì‹¤ì œ ì‚¬ìš©ì í”¼ë“œë°± ë°ì´í„° ìˆ˜ì§‘"""
        try:
            self.logger.info("ğŸ“Š ì‹¤ì œ ì‚¬ìš©ì í”¼ë“œë°± ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‹¤ì œ í”¼ë“œë°± ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            db = get_database_manager()
            feedback_data = db.get_recent_feedback(limit=100)
            
            texts = []
            labels = []
            
            if feedback_data:
                self.logger.info(f"âœ… {len(feedback_data)}ê°œì˜ ì‹¤ì œ í”¼ë“œë°± ë°œê²¬")
                
                for feedback in feedback_data:
                    try:
                        # í”„ë¡œê·¸ë¨ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                        program = db.get_program_by_external_id(feedback.get('program_external_id', ''))
                        if not program:
                            continue
                            
                        # í…ìŠ¤íŠ¸ êµ¬ì„±
                        title = program.get('title', '')
                        content = program.get('content', '')
                        text = f"{title} {content}".strip()
                        
                        if not text:
                            continue
                        
                        # í”¼ë“œë°± ì•¡ì…˜ì„ ë¼ë²¨ë¡œ ë³€í™˜
                        action = feedback.get('action', '')
                        if action in ['keep', 'like', 'interest']:
                            label = 1  # ê¸ì •
                        elif action in ['delete', 'dislike', 'not_interested']:
                            label = 0  # ë¶€ì •
                        else:
                            continue
                        
                        texts.append(text)
                        labels.append(label)
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ í”¼ë“œë°± ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                        
                self.logger.info(f"ğŸ“ˆ ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°: {len(texts)}ê°œ (ê¸ì •: {sum(labels)}, ë¶€ì •: {len(labels)-sum(labels)})")
                
            # ì‹¤ì œ í”¼ë“œë°±ì´ ë¶€ì¡±í•œ ê²½ìš° ìµœì†Œí•œì˜ ê¸°ë³¸ ì¼€ì´ìŠ¤ ì¶”ê°€
            if len(texts) < 10:
                self.logger.warning("âš ï¸ ì‹¤ì œ í”¼ë“œë°± ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ê¸°ë³¸ ì¼€ì´ìŠ¤ ì¶”ê°€...")
                additional_cases = [
                    ('ìŠ¤íƒ€íŠ¸ì—… í€ë”© ì§€ì›ì‚¬ì—… ê³µëª¨', 1),
                    ('ì°½ì—…ê¸°ì—… ìœ¡ì„± í”„ë¡œê·¸ë¨ ëª¨ì§‘', 1),
                    ('ë²¤ì²˜íˆ¬ì ë§¤ì¹­ ì„œë¹„ìŠ¤', 1),
                    ('ì¼ë°˜ ë§ˆì¼€íŒ… ê´‘ê³ ', 0),
                    ('ë¶€ë™ì‚° íˆ¬ì ì•ˆë‚´', 0),
                    ('ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ì˜¤í”ˆ', 0),
                ]
                
                for text, label in additional_cases:
                    texts.append(text)
                    labels.append(label)
                    
                self.logger.info(f"ğŸ“ ê¸°ë³¸ ì¼€ì´ìŠ¤ {len(additional_cases)}ê°œ ì¶”ê°€")
                
            self.logger.info(f"ğŸ¯ ìµœì¢… í•™ìŠµ ë°ì´í„°: {len(texts)}ê°œ")
            return texts, labels
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ í”¼ë“œë°± ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.logger.info("ğŸ”„ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ í´ë°±...")
            
            # í´ë°± ì¼€ì´ìŠ¤
            fallback_cases = [
                ('ìŠ¤íƒ€íŠ¸ì—… í€ë”© ì§€ì›ì‚¬ì—… ê³µëª¨', 1),
                ('ì°½ì—…ê¸°ì—… ìœ¡ì„± í”„ë¡œê·¸ë¨ ëª¨ì§‘', 1),
                ('ë²¤ì²˜íˆ¬ì ë§¤ì¹­ ì„œë¹„ìŠ¤', 1),
                ('ì¼ë°˜ ë§ˆì¼€íŒ… ê´‘ê³ ', 0),
                ('ë¶€ë™ì‚° íˆ¬ì ì•ˆë‚´', 0),
                ('ì˜¨ë¼ì¸ ì‡¼í•‘ëª° ì˜¤í”ˆ', 0),
            ]
            
            texts = [case[0] for case in fallback_cases]
            labels = [case[1] for case in fallback_cases]
            
            return texts, labels
    
    def _create_dummy_model(self):
        """ì‹¤ì œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ ì‹œë„, ì‹¤íŒ¨ì‹œ ë”ë¯¸ ëª¨ë¸"""
        try:
            # ì‹¤ì œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ ì‹œë„
            from .deep_learning_engine import get_deep_learning_engine
            self.logger.info("ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ ì‹œë„...")
            
            deep_engine = get_deep_learning_engine()
            if deep_engine and hasattr(deep_engine, 'predict'):
                self.logger.info("âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                
                # ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ê°•í™”í•™ìŠµì— ë§ê²Œ ë˜í•‘
                class DeepLearningWrapper:
                    def __init__(self, engine):
                        self.engine = engine
                        self.threshold = 0.5
                        self.model_weights = {'deep_learning': 1.0}
                    
                    def predict(self, texts):
                        if isinstance(texts, str):
                            texts = [texts]
                        
                        predictions = []
                        for text in texts:
                            try:
                                score = self.engine.calculate_score(text)
                                predictions.append(1 if score > 50 else 0)
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
                                predictions.append(0)
                        return predictions
                
                return DeepLearningWrapper(deep_engine)
            else:
                self.logger.warning("âš ï¸ ë”¥ëŸ¬ë‹ ì—”ì§„ì´ ì—†ê±°ë‚˜ ì˜ˆì¸¡ ë©”ì„œë“œê°€ ì—†ìŒ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.logger.info("ğŸ’¡ ë”ë¯¸ ëª¨ë¸ë¡œ í´ë°±...")
            
        # ë”ë¯¸ ëª¨ë¸ ìƒì„±
        class DummyModel:
            def __init__(self):
                self.threshold = 0.5
                self.model_weights = {'dummy': 1.0}
            
            def predict(self, texts):
                if isinstance(texts, str):
                    texts = [texts]
                    
                predictions = []
                for text in texts:
                    if any(keyword in text.lower() for keyword in ['ìŠ¤íƒ€íŠ¸ì—…', 'ì°½ì—…', 'í€ë”©', 'íˆ¬ì', 'ì§€ì›ì‚¬ì—…']):
                        predictions.append(1)
                    else:
                        predictions.append(0)
                return predictions
        
        self.logger.info("ğŸ”„ ë”ë¯¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        return DummyModel()
    
    def _create_dummy_test_data(self) -> Tuple[List[str], List[int]]:
        """ë”ë¯¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        texts = [
            'ìŠ¤íƒ€íŠ¸ì—… ì§€ì›ì‚¬ì—… ê³µê³ ',
            'ì°½ì—…ê¸°ì—… í€ë”© ëª¨ì§‘',
            'ì¼ë°˜ ê´‘ê³  ë‚´ìš©',
            'ë¶€ë™ì‚° íˆ¬ì ì•ˆë‚´'
        ]
        labels = [1, 1, 0, 0]
        return texts, labels
    
    def _evaluate_model(self) -> Dict[str, float]:
        """ëª¨ë¸ ì„±ê³¼ í‰ê°€"""
        try:
            predictions = []
            
            # ê° í…ìŠ¤íŠ¸ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì˜ˆì¸¡
            for text in self.test_texts:
                try:
                    # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                    text_str = str(text) if not isinstance(text, str) else text
                    pred = self.model.predict(text_str)
                    
                    # ì˜ˆì¸¡ê°’ì´ ì ìˆ˜ì¸ ê²½ìš° 0/1ë¡œ ë³€í™˜ (50ì  ì´ìƒì´ë©´ 1)
                    if isinstance(pred, (int, float)):
                        predictions.append(1 if pred > 50 else 0)
                    else:
                        predictions.append(int(pred) if pred else 0)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ê°œë³„ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    predictions.append(0)
            
            # ê¸¸ì´ ë§ì¶”ê¸°
            if len(predictions) != len(self.test_labels):
                self.logger.warning(f"âš ï¸ ì˜ˆì¸¡/ë¼ë²¨ ê¸¸ì´ ë¶ˆì¼ì¹˜: {len(predictions)} vs {len(self.test_labels)}")
                predictions = predictions[:len(self.test_labels)]
                if len(predictions) < len(self.test_labels):
                    predictions.extend([0] * (len(self.test_labels) - len(predictions)))
            
            accuracy = accuracy_score(self.test_labels, predictions)
            precision = precision_score(self.test_labels, predictions, zero_division=0)
            recall = recall_score(self.test_labels, predictions, zero_division=0)
            f1 = f1_score(self.test_labels, predictions, zero_division=0)
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
        except Exception as e:
            self.logger.error(f"âš ï¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
    
    def optimize_threshold_rl(self, total_timesteps: int = 10000):
        """ê°•í™”í•™ìŠµìœ¼ë¡œ ì„ê³„ê°’ ìµœì í™”"""
        self.logger.info("\nğŸ¯ ê°•í™”í•™ìŠµ ì„ê³„ê°’ ìµœì í™” ì‹œì‘!")
        self.logger.info("-" * 40)
        
        # í™˜ê²½ ìƒì„±
        env = StartupClassifierEnv(self.model, self.test_texts, self.test_labels, mode='threshold')
        # Stable-baselines3 í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ì ìš©
        env = StableBaselinesEnvWrapper(env)
        
        # PPO ì—ì´ì „íŠ¸ í›ˆë ¨
        model = PPO("MlpPolicy", env, verbose=1, learning_rate=0.001)
        
        self.logger.info("ğŸ§  PPO ì—ì´ì „íŠ¸ í›ˆë ¨ ì¤‘...")
        model.learn(total_timesteps=total_timesteps)
        
        # ìµœì  ì„ê³„ê°’ ì°¾ê¸°
        obs = env.reset()
        best_threshold = 0.5
        best_score = 0
        
        for _ in range(100):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, _ = env.step(action)
            
            current_performance = self._evaluate_model()
            if current_performance['accuracy'] > best_score:
                best_score = current_performance['accuracy']
                best_threshold = self.model.threshold
        
        # ìµœì  ê°’ ì ìš©
        self.model.threshold = best_threshold
        optimized_performance = self._evaluate_model()
        
        self.logger.info(f"\nâœ… ì„ê³„ê°’ ìµœì í™” ì™„ë£Œ!")
        self.logger.info(f"   ìµœì  ì„ê³„ê°’: {best_threshold:.3f}")
        self.logger.info(f"   ê°œì„  ì „: {self.baseline_performance['accuracy']:.3f}")
        self.logger.info(f"   ê°œì„  í›„: {optimized_performance['accuracy']:.3f}")
        self.logger.info(f"   í–¥ìƒë„: +{optimized_performance['accuracy'] - self.baseline_performance['accuracy']:.3f}")
        
        return best_threshold, optimized_performance
    
    def optimize_weights_rl(self, total_timesteps: int = 10000):
        """ê°•í™”í•™ìŠµìœ¼ë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ìµœì í™”"""
        self.logger.info("\nâš–ï¸ ê°•í™”í•™ìŠµ ê°€ì¤‘ì¹˜ ìµœì í™” ì‹œì‘!")
        self.logger.info("-" * 40)
        
        # í™˜ê²½ ìƒì„±
        env = StartupClassifierEnv(self.model, self.test_texts, self.test_labels, mode='weights')
        # Stable-baselines3 í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ì ìš©
        env = StableBaselinesEnvWrapper(env)
        
        # A2C ì—ì´ì „íŠ¸ í›ˆë ¨ (ì—°ì†ì  ì•¡ì…˜ì— ì í•©)
        model = A2C("MlpPolicy", env, verbose=1, learning_rate=0.001)
        
        self.logger.info("ğŸ§  A2C ì—ì´ì „íŠ¸ í›ˆë ¨ ì¤‘...")
        model.learn(total_timesteps=total_timesteps)
        
        # ìµœì  ê°€ì¤‘ì¹˜ ì°¾ê¸°
        obs = env.reset()
        best_weights = self.model.model_weights.copy()
        best_score = 0
        
        for _ in range(100):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, _ = env.step(action)
            
            current_performance = self._evaluate_model()
            if current_performance['accuracy'] > best_score:
                best_score = current_performance['accuracy']
                best_weights = self.model.model_weights.copy()
        
        # ìµœì  ê°€ì¤‘ì¹˜ ì ìš©
        self.model.model_weights = best_weights
        optimized_performance = self._evaluate_model()
        
        self.logger.info(f"\nâœ… ê°€ì¤‘ì¹˜ ìµœì í™” ì™„ë£Œ!")
        self.logger.info(f"   ìµœì  ê°€ì¤‘ì¹˜: {best_weights}")
        self.logger.info(f"   ê°œì„  ì „: {self.baseline_performance['accuracy']:.3f}")
        self.logger.info(f"   ê°œì„  í›„: {optimized_performance['accuracy']:.3f}")
        self.logger.info(f"   í–¥ìƒë„: +{optimized_performance['accuracy'] - self.baseline_performance['accuracy']:.3f}")
        
        return best_weights, optimized_performance
    
    def hyperparameter_search_optuna(self, n_trials: int = 100):
        """Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        self.logger.info("\nğŸ” Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”!")
        self.logger.info("-" * 40)
        
        def objective(trial):
            # ì„ê³„ê°’ê³¼ ê°€ì¤‘ì¹˜ ë™ì‹œ ìµœì í™”
            threshold = trial.suggest_float('threshold', 0.2, 0.8)
            
            # ê° ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜
            weights = {}
            model_names = list(self.model.model_weights.keys())
            for name in model_names:
                weights[name] = trial.suggest_float(f'weight_{name}', 0.1, 1.0)
            
            # ì„ì‹œ ì ìš©
            original_threshold = self.model.threshold
            original_weights = self.model.model_weights.copy()
            
            try:
                self.model.threshold = threshold
                self.model.model_weights = weights
                
                performance = self._evaluate_model()
                
                # ë³µí•© ì ìˆ˜ (ì •í™•ë„ + ì¬í˜„ìœ¨ ì¤‘ì‹œ)
                score = 0.6 * performance['accuracy'] + 0.4 * performance['recall']
                
            except Exception as e:
                score = 0.0
            finally:
                # ì›ë˜ ê°’ ë³µì›
                self.model.threshold = original_threshold
                self.model.model_weights = original_weights
            
            return score
        
        # Optuna ìŠ¤í„°ë”” ì‹¤í–‰
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì ìš©
        best_params = study.best_params
        self.model.threshold = best_params['threshold']
        
        for name in self.model.model_weights.keys():
            self.model.model_weights[name] = best_params[f'weight_{name}']
        
        final_performance = self._evaluate_model()
        
        self.logger.info(f"\nâœ… Optuna ìµœì í™” ì™„ë£Œ!")
        self.logger.info(f"   ìµœì  ì„ê³„ê°’: {best_params['threshold']:.3f}")
        self.logger.info(f"   ìµœì  ì ìˆ˜: {study.best_value:.3f}")
        self.logger.info(f"   ìµœì¢… ì •í™•ë„: {final_performance['accuracy']:.3f}")
        
        return best_params, final_performance
    
    def detailed_analysis(self):
        """ìƒì„¸ ë¶„ì„ ë° ë¬¸ì œ ì¼€ì´ìŠ¤ ì¬ê²€í† """
        self.logger.info("\nğŸ” ìƒì„¸ ë¶„ì„ ì‹œì‘!")
        self.logger.info("="*50)
        
        # í˜„ì¬ ì„±ê³¼
        current_performance = self._evaluate_model()
        predictions = self.model.predict(self.test_texts)
        
        self.logger.info("ğŸ“Š ì „ì²´ ì„±ê³¼:")
        for metric, value in current_performance.items():
            self.logger.info(f"   {metric}: {value:.3f}")
        
        # ë¬¸ì œ ì¼€ì´ìŠ¤ ë¶„ì„
        problem_cases = []
        for i, (text, true_label, pred) in enumerate(zip(self.test_texts, self.test_labels, predictions)):
            if true_label != pred:
                problem_cases.append({
                    'text': text,
                    'true_label': 'ì§€ì›ì‚¬ì—…' if true_label == 1 else 'ì¼ë°˜',
                    'predicted': 'ì§€ì›ì‚¬ì—…' if pred == 1 else 'ì¼ë°˜',
                    'type': 'FN' if true_label == 1 and pred == 0 else 'FP'
                })
        
        self.logger.info(f"\nâŒ ë¬¸ì œ ì¼€ì´ìŠ¤ {len(problem_cases)}ê°œ:")
        for i, case in enumerate(problem_cases):
            self.logger.info(f"   {i+1}. [{case['type']}] {case['text'][:50]}...")
            self.logger.info(f"      ì‹¤ì œ: {case['true_label']} | ì˜ˆì¸¡: {case['predicted']}")
        
        return current_performance, problem_cases
    
    def save_optimized_model(self, filename: str = 'rl_optimized_model.pkl'):
        """ìµœì í™”ëœ ëª¨ë¸ ì €ì¥"""
        self.logger.info(f"\nğŸ’¾ ìµœì í™”ëœ ëª¨ë¸ ì €ì¥: {filename}")
        
        with open(filename, 'wb') as f:
            pickle.dump(self.model, f)
        
        # ìµœì í™” ê²°ê³¼ ë©”íƒ€ë°ì´í„°
        metadata = {
            'optimization_method': 'reinforcement_learning',
            'baseline_accuracy': self.baseline_performance['accuracy'],
            'optimized_accuracy': self._evaluate_model()['accuracy'],
            'threshold': getattr(self.model, 'threshold', 0.5),
            'model_weights': getattr(self.model, 'model_weights', {}),
            'test_cases': len(self.test_texts)
        }
        
        with open(f'rl_optimization_results.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        self.logger.info("âœ… ì €ì¥ ì™„ë£Œ!")

    def optimize_from_feedback(self, feedback_data: List[Dict]) -> Dict[str, Any]:
        """í”¼ë“œë°± ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ìµœì í™”"""
        try:
            self.logger.info(f"\nğŸ¯ í”¼ë“œë°± ê¸°ë°˜ ê°•í™”í•™ìŠµ ì‹œì‘: {len(feedback_data)}ê°œ ë°ì´í„°")
            self.logger.info("-" * 50)
            
            # í”¼ë“œë°± ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ë³€í™˜
            texts, labels = self._convert_feedback_to_training_data(feedback_data)
            
            if len(texts) < 3:
                return {
                    'status': 'insufficient_data',
                    'message': 'í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.',
                    'data_count': len(texts)
                }
            
            # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— í”¼ë“œë°± ë°ì´í„° ì¶”ê°€
            combined_texts = self.test_texts + texts
            combined_labels = self.test_labels + labels
            
            # ì„±ëŠ¥ í‰ê°€ ì „ í˜„ì¬ ìƒíƒœ ì €ì¥
            original_performance = self._evaluate_model()
            
            # ì„ê³„ê°’ ìµœì í™” (ì‘ì€ ìŠ¤ì¼€ì¼)
            self.logger.info("ğŸ¯ í”¼ë“œë°± ê¸°ë°˜ ì„ê³„ê°’ ìµœì í™”...")
            optimized_threshold, threshold_performance = self._optimize_threshold_from_feedback(
                combined_texts, combined_labels
            )
            
            # ê°€ì¤‘ì¹˜ ìµœì í™” (ì‘ì€ ìŠ¤ì¼€ì¼)
            self.logger.info("âš–ï¸ í”¼ë“œë°± ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™”...")
            optimized_weights, weights_performance = self._optimize_weights_from_feedback(
                combined_texts, combined_labels
            )
            
            # ìµœì í™” ê²°ê³¼ ì €ì¥
            optimization_result = {
                'status': 'success',
                'original_performance': original_performance,
                'threshold_optimization': {
                    'optimal_threshold': optimized_threshold,
                    'performance': threshold_performance
                },
                'weights_optimization': {
                    'optimal_weights': optimized_weights,
                    'performance': weights_performance
                },
                'improvement': {
                    'accuracy_gain': weights_performance['accuracy'] - original_performance['accuracy'],
                    'f1_gain': weights_performance['f1'] - original_performance['f1']
                },
                'feedback_processed': len(feedback_data),
                'training_samples': len(texts),
                'timestamp': datetime.now().isoformat()
            }
            
            self.logger.info(f"âœ… í”¼ë“œë°± ê¸°ë°˜ ìµœì í™” ì™„ë£Œ!")
            self.logger.info(f"   ì •í™•ë„ í–¥ìƒ: {optimization_result['improvement']['accuracy_gain']:.3f}")
            self.logger.info(f"   F1 ìŠ¤ì½”ì–´ í–¥ìƒ: {optimization_result['improvement']['f1_gain']:.3f}")
            
            # ì„±ëŠ¥ì´ í–¥ìƒëœ ê²½ìš°ì—ë§Œ ì ìš©
            if optimization_result['improvement']['accuracy_gain'] > 0:
                self.model.threshold = optimized_threshold
                if hasattr(self.model, 'model_weights'):
                    self.model.model_weights = optimized_weights
                self.logger.info("ğŸ‰ ìµœì í™”ëœ ì„¤ì • ì ìš© ì™„ë£Œ!")
            else:
                self.logger.warning("âš ï¸ ì„±ëŠ¥ í–¥ìƒì´ ì—†ì–´ ê¸°ì¡´ ì„¤ì • ìœ ì§€")
            
            return optimization_result
            
        except Exception as e:
            self.logger.error(f"âŒ í”¼ë“œë°± ê¸°ë°˜ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                'status': 'error',
                'message': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _convert_feedback_to_training_data(self, feedback_data: List[Dict]) -> Tuple[List[str], List[int]]:
        """í”¼ë“œë°± ë°ì´í„°ë¥¼ í•™ìŠµìš© ë°ì´í„°ë¡œ ë³€í™˜"""
        texts = []
        labels = []
        
        for feedback in feedback_data:
            try:
                title = feedback.get('title', '')
                content = feedback.get('content', '')
                action = feedback.get('action', '')
                
                # í…ìŠ¤íŠ¸ êµ¬ì„±
                text = f"{title} {content}".strip()
                if not text:
                    continue
                
                # ë¼ë²¨ ë³€í™˜ (keep=1, delete=0)
                if action in ['keep', 'like', 'interest']:
                    label = 1
                elif action in ['delete', 'dislike', 'uninterest']:
                    label = 0
                else:
                    continue  # ì•Œ ìˆ˜ ì—†ëŠ” ì•¡ì…˜ì€ ê±´ë„ˆë›°ê¸°
                
                texts.append(text)
                labels.append(label)
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ í”¼ë“œë°± ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue
        
        self.logger.info(f"ğŸ“Š í”¼ë“œë°± ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(texts)}ê°œ (Keep: {sum(labels)}, Delete: {len(labels)-sum(labels)})")
        return texts, labels
    
    def _optimize_threshold_from_feedback(self, texts: List[str], labels: List[int]) -> Tuple[float, Dict[str, float]]:
        """í”¼ë“œë°± ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’ ìµœì í™” (ê°„ì†Œí™” ë²„ì „)"""
        try:
            best_threshold = self.model.threshold if hasattr(self.model, 'threshold') else 0.5
            best_performance = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}
            
            # ì„ê³„ê°’ ë²”ìœ„ íƒìƒ‰ (0.3 ~ 0.8)
            for threshold in np.arange(0.3, 0.81, 0.05):
                try:
                    # ì„ê³„ê°’ ì„ì‹œ ì ìš©
                    if hasattr(self.model, 'threshold'):
                        self.model.threshold = threshold
                    
                    # ì„±ëŠ¥ í‰ê°€
                    predictions = self.model.predict(texts)
                    
                    performance = {
                        'accuracy': accuracy_score(labels, predictions),
                        'precision': precision_score(labels, predictions, zero_division=0),
                        'recall': recall_score(labels, predictions, zero_division=0),
                        'f1': f1_score(labels, predictions, zero_division=0)
                    }
                    
                    # ìµœì  ì„ê³„ê°’ ì—…ë°ì´íŠ¸
                    if performance['f1'] > best_performance['f1']:
                        best_threshold = threshold
                        best_performance = performance
                        
                except Exception as e:
                    continue
            
            return best_threshold, best_performance
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„ê³„ê°’ ìµœì í™” ì‹¤íŒ¨: {e}")
            return 0.5, {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}
    
    def _optimize_weights_from_feedback(self, texts: List[str], labels: List[int]) -> Tuple[Dict[str, float], Dict[str, float]]:
        """í”¼ë“œë°± ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™” (ê°„ì†Œí™” ë²„ì „)"""
        try:
            if not hasattr(self.model, 'model_weights'):
                # ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ëª¨ë¸ì˜ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
                default_weights = {'model1': 1.0, 'model2': 1.0, 'model3': 1.0}
                performance = self._evaluate_model_on_data(texts, labels)
                return default_weights, performance
            
            best_weights = self.model.model_weights.copy()
            best_performance = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}
            
            # ê°€ì¤‘ì¹˜ ì¡°í•© íƒìƒ‰ (ê°„ì†Œí™”)
            weight_combinations = [
                {'model1': 1.0, 'model2': 0.8, 'model3': 0.6},
                {'model1': 0.8, 'model2': 1.0, 'model3': 0.8},
                {'model1': 0.6, 'model2': 0.8, 'model3': 1.0},
                {'model1': 1.2, 'model2': 1.0, 'model3': 0.8},
                {'model1': 0.8, 'model2': 1.2, 'model3': 1.0}
            ]
            
            for weights in weight_combinations:
                try:
                    # ê°€ì¤‘ì¹˜ ì„ì‹œ ì ìš©
                    self.model.model_weights = weights
                    
                    # ì„±ëŠ¥ í‰ê°€
                    performance = self._evaluate_model_on_data(texts, labels)
                    
                    # ìµœì  ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                    if performance['f1'] > best_performance['f1']:
                        best_weights = weights.copy()
                        best_performance = performance
                        
                except Exception as e:
                    continue
            
            return best_weights, best_performance
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ê°€ì¤‘ì¹˜ ìµœì í™” ì‹¤íŒ¨: {e}")
            default_weights = {'model1': 1.0, 'model2': 1.0, 'model3': 1.0}
            performance = self._evaluate_model_on_data(texts, labels)
            return default_weights, performance
    
    def _evaluate_model_on_data(self, texts: List[str], labels: List[int]) -> Dict[str, float]:
        """íŠ¹ì • ë°ì´í„°ì— ëŒ€í•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        try:
            predictions = self.model.predict(texts)
            
            return {
                'accuracy': accuracy_score(labels, predictions),
                'precision': precision_score(labels, predictions, zero_division=0),
                'recall': recall_score(labels, predictions, zero_division=0),
                'f1': f1_score(labels, predictions, zero_division=0)
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}
    
    def get_rl_optimization_status(self) -> Dict[str, Any]:
        """ê°•í™”í•™ìŠµ ìµœì í™” ìƒíƒœ ì¡°íšŒ"""
        try:
            current_performance = self._evaluate_model()
            
            status = {
                'optimizer_initialized': True,
                'baseline_performance': self.baseline_performance,
                'current_performance': current_performance,
                'improvement': {
                    'accuracy_gain': current_performance['accuracy'] - self.baseline_performance['accuracy'],
                    'f1_gain': current_performance['f1'] - self.baseline_performance['f1']
                },
                'model_config': {
                    'threshold': getattr(self.model, 'threshold', 0.5),
                    'weights': getattr(self.model, 'model_weights', {}),
                },
                'test_data_size': len(self.test_texts),
                'last_updated': datetime.now().isoformat()
            }
            
            return status
            
        except Exception as e:
            return {
                'optimizer_initialized': False,
                'error': str(e),
                'last_updated': datetime.now().isoformat()
            }

    def retrain_deep_learning_model(self, feedback_data: List[Dict]) -> Dict[str, Any]:
        """ì‹¤ì œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¬í›ˆë ¨ - pkl ëª¨ë¸ì´ ì§„ì§œë¡œ ë˜‘ë˜‘í•´ì§!"""
        try:
            self.logger.info("\nğŸ§  ì‹¤ì œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¬í›ˆë ¨ ì‹œì‘!")
            self.logger.info("="*50)
            
            # í”¼ë“œë°± ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ë³€í™˜
            texts, labels = self._convert_feedback_to_training_data(feedback_data)
            
            if len(texts) < 5:
                return {
                    'status': 'insufficient_data',
                    'message': f'ì¬í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (í˜„ì¬: {len(texts)}ê°œ, í•„ìš”: 5ê°œ)',
                    'data_count': len(texts)
                }
            
            self.logger.info(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {len(texts)}ê°œ (ê¸ì •: {sum(labels)}, ë¶€ì •: {len(labels)-sum(labels)})")
            
            # AIEngineì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì°¾ê¸°
            deep_model = None
            feature_extractor = None
            
            # AIEngineì˜ model_managerì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì°¾ê¸°
            if hasattr(self.model, 'model_manager') and hasattr(self.model.model_manager, 'deep_learning_model'):
                deep_learning_engine = self.model.model_manager.deep_learning_model
                if deep_learning_engine and hasattr(deep_learning_engine, 'model') and hasattr(deep_learning_engine, 'feature_extractor'):
                    deep_model = deep_learning_engine.model
                    feature_extractor = deep_learning_engine.feature_extractor
                    self.logger.info("âœ… AIEngineì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë°œê²¬")
            
            # ì§ì ‘ ë”¥ëŸ¬ë‹ ì—”ì§„ì—ì„œ ì°¾ê¸° (ë°±ì—…)
            if deep_model is None:
                try:
                    from .deep_learning_engine import get_deep_learning_engine
                    deep_learning_engine = get_deep_learning_engine()
                    if deep_learning_engine and hasattr(deep_learning_engine, 'model') and hasattr(deep_learning_engine, 'feature_extractor'):
                        deep_model = deep_learning_engine.model
                        feature_extractor = deep_learning_engine.feature_extractor
                        self.logger.info("âœ… ì§ì ‘ ë”¥ëŸ¬ë‹ ì—”ì§„ì—ì„œ ëª¨ë¸ ë°œê²¬")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì§ì ‘ ë”¥ëŸ¬ë‹ ì—”ì§„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            if deep_model is None or feature_extractor is None:
                self.logger.warning("âš ï¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë˜ëŠ” íŠ¹ì„± ì¶”ì¶œê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return {'status': 'no_model', 'message': 'ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
            
            self.logger.info("âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ ë°œê²¬ - ì‹¤ì œ ì¬í›ˆë ¨ ì‹œì‘")
            
            # ì„±ëŠ¥ í‰ê°€ ì „
            before_performance = self._evaluate_model()
            self.logger.info(f"ğŸ¯ ì¬í›ˆë ¨ ì „ ì„±ëŠ¥: ì •í™•ë„ {before_performance['accuracy']:.3f}")
            
            # ì‹¤ì œ ëª¨ë¸ ì¬í›ˆë ¨ ì‹¤í–‰
            retrain_result = self._perform_actual_retraining(
                deep_model, feature_extractor, texts, labels
            )
            
            if retrain_result['success']:
                # ì„±ëŠ¥ í‰ê°€ í›„
                after_performance = self._evaluate_model()
                self.logger.info(f"ğŸ‰ ì¬í›ˆë ¨ í›„ ì„±ëŠ¥: ì •í™•ë„ {after_performance['accuracy']:.3f}")
                
                improvement = after_performance['accuracy'] - before_performance['accuracy']
                self.logger.info(f"ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ: {improvement:+.3f}")
                
                # í–¥ìƒëœ ëª¨ë¸ ì €ì¥
                self._save_improved_model()
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— í•™ìŠµ ì´ë²¤íŠ¸ ê¸°ë¡
                try:
                    if hasattr(self, 'ai_engine') and self.ai_engine and hasattr(self.ai_engine, 'db_manager'):
                        self.ai_engine.db_manager.record_learning_event(
                            learning_type='deep_learning_retrain',
                            performance_before=before_performance,
                            performance_after=after_performance,
                            details={
                                'training_samples': len(texts),
                                'epochs_trained': retrain_result.get('epochs', 0),
                                'final_loss': retrain_result.get('final_loss', 0),
                                'improvement': improvement
                            }
                        )
                        self.logger.info(f"ğŸ“Š DBì— í•™ìŠµ í†µê³„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ DB í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                return {
                    'status': 'success',
                    'message': 'ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¬í›ˆë ¨ ì™„ë£Œ!',
                    'before_performance': before_performance,
                    'after_performance': after_performance,
                    'improvement': improvement,
                    'training_samples': len(texts),
                    'epochs_trained': retrain_result.get('epochs', 0),
                    'final_loss': retrain_result.get('final_loss', 0),
                    'timestamp': datetime.now().isoformat()
                }
            else:
                return {
                    'status': 'training_failed',
                    'message': retrain_result.get('error', 'ì¬í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ'),
                    'timestamp': datetime.now().isoformat()
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¬í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return {
                'status': 'error',
                'message': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _perform_actual_retraining(self, model, feature_extractor, texts: List[str], labels: List[int]) -> Dict[str, Any]:
        """ì‹¤ì œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        try:
            import torch
            import torch.nn as nn
            import torch.optim as optim
            from torch.utils.data import DataLoader, TensorDataset
            
            self.logger.info("ğŸ”¥ PyTorch ì¬í›ˆë ¨ ì‹œì‘...")
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì • - Configì—ì„œ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            from .config import Config
            device = Config.DEVICE  # MPS, CUDA, ë˜ëŠ” CPU ìë™ ì„ íƒ
            
            self.logger.info(f"ğŸ–¥ï¸ í•™ìŠµ ë””ë°”ì´ìŠ¤: {device} ({Config.DEVICE_NAME})")
            
            # íŠ¹ì„± ì¶”ì¶œ
            self.logger.info("ğŸ”§ í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
            features = feature_extractor.extract_features(texts, is_training=True)
            
            # PyTorch í…ì„œë¡œ ë³€í™˜
            X_tensor = torch.FloatTensor(features).to(device)
            y_tensor = torch.FloatTensor(labels).to(device)
            
            self.logger.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„° í˜•íƒœ: {X_tensor.shape}")
            
            # ë°ì´í„° ë¡œë” ìƒì„±
            dataset = TensorDataset(X_tensor, y_tensor)
            dataloader = DataLoader(dataset, batch_size=min(16, len(texts)), shuffle=True)
            
            # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
            model.train()
            
            # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ë‚®ì€ í•™ìŠµë¥ ë¡œ íŒŒì¸íŠœë‹)
            optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)
            criterion = nn.BCEWithLogitsLoss()
            
            # í•™ìŠµ ì‹¤í–‰
            num_epochs = min(10, max(3, len(texts) // 2))  # ë°ì´í„° ì–‘ì— ë”°ë¼ ì¡°ì •
            self.logger.info(f"ğŸ“š {num_epochs} ì—í¬í¬ ë™ì•ˆ íŒŒì¸íŠœë‹...")
            
            total_loss = 0
            for epoch in range(num_epochs):
                epoch_loss = 0
                for batch_X, batch_y in dataloader:
                    optimizer.zero_grad()
                    
                    # ìˆœì „íŒŒ
                    outputs = model(batch_X)
                    loss = criterion(outputs.squeeze(), batch_y)
                    
                    # ì—­ì „íŒŒ
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                
                avg_epoch_loss = epoch_loss / len(dataloader)
                total_loss += avg_epoch_loss
                self.logger.info(f"   ì—í¬í¬ {epoch+1}/{num_epochs}: Loss = {avg_epoch_loss:.4f}")
            
            final_loss = total_loss / num_epochs
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
            model.eval()
            
            self.logger.info(f"âœ… ì¬í›ˆë ¨ ì™„ë£Œ! í‰ê·  Loss: {final_loss:.4f}")
            
            return {
                'success': True,
                'epochs': num_epochs,
                'final_loss': final_loss,
                'samples_trained': len(texts)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ì¬í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _save_improved_model(self):
        """í–¥ìƒëœ ëª¨ë¸ ì €ì¥"""
        try:
            import pickle
            from datetime import datetime
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ë°±ì—… íŒŒì¼ëª…
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"models/rl_improved_model_{timestamp}.pkl"
            
            self.logger.info(f"ğŸ’¾ í–¥ìƒëœ ëª¨ë¸ ì €ì¥ ì¤‘: {backup_path}")
            
            # AIEngineì˜ model_managerì—ì„œ ë”¥ëŸ¬ë‹ ì—”ì§„ ì°¾ê¸°
            deep_learning_engine = None
            if hasattr(self.model, 'model_manager') and hasattr(self.model.model_manager, 'deep_learning_model'):
                deep_learning_engine = self.model.model_manager.deep_learning_model
            
            # ì§ì ‘ ë”¥ëŸ¬ë‹ ì—”ì§„ì—ì„œ ì°¾ê¸° (ë°±ì—…)
            if deep_learning_engine is None:
                try:
                    from .deep_learning_engine import get_deep_learning_engine
                    deep_learning_engine = get_deep_learning_engine()
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë”¥ëŸ¬ë‹ ì—”ì§„ ì°¾ê¸° ì‹¤íŒ¨: {e}")
                    deep_learning_engine = self.model  # AIEngine ìì²´ë¥¼ ì €ì¥
            
            # ëª¨ë¸ ì €ì¥
            with open(backup_path, 'wb') as f:
                pickle.dump(deep_learning_engine, f)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'improvement_type': 'reinforcement_learning_feedback',
                'improved_at': datetime.now().isoformat(),
                'baseline_accuracy': self.baseline_performance.get('accuracy', 0),
                'current_accuracy': self._evaluate_model().get('accuracy', 0),
                'training_samples': len(self.test_texts),
                'model_path': backup_path
            }
            
            metadata_path = f"models/rl_improvement_log_{timestamp}.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                import json
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {backup_path}")
            self.logger.info(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger = get_logger(__name__)
    logger.info("ğŸš€ ìŠ¤íƒ€íŠ¸ì—… ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ - ê°•í™”í•™ìŠµ ìµœì í™”")
    logger.info("="*60)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì½”ë©ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ë“¤)
    model_path = 'improved_ensemble_model.pkl'  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸
    test_data_path = 'test_data.json'
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        logger.info("ğŸ’¡ ì½”ë©ì—ì„œ 14-15ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì—¬ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”!")
        return
    
    if not os.path.exists(test_data_path):
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_data_path}")
        return
    
    # ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    optimizer = ReinforcementLearningOptimizer(model_path, test_data_path)
    
    # 1. ë² ì´ìŠ¤ë¼ì¸ ë¶„ì„
    logger.info("\n" + "="*60)
    optimizer.detailed_analysis()
    
    # 2. Optuna ìµœì í™” (ë¹ ë¥´ê³  íš¨ê³¼ì )
    logger.info("\n" + "="*60)
    best_params, performance = optimizer.hyperparameter_search_optuna(n_trials=50)
    
    # 3. ê°•í™”í•™ìŠµ ì„ê³„ê°’ ìµœì í™”
    logger.info("\n" + "="*60)
    threshold, threshold_performance = optimizer.optimize_threshold_rl(total_timesteps=5000)
    
    # 4. ê°•í™”í•™ìŠµ ê°€ì¤‘ì¹˜ ìµœì í™”  
    logger.info("\n" + "="*60)
    weights, weights_performance = optimizer.optimize_weights_rl(total_timesteps=5000)
    
    # 5. ìµœì¢… ë¶„ì„
    logger.info("\n" + "="*60)
    final_performance, final_problems = optimizer.detailed_analysis()
    
    # 6. ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
    optimizer.save_optimized_model()
    
    logger.info(f"\nğŸ‰ ê°•í™”í•™ìŠµ ìµœì í™” ì™„ë£Œ!")
    logger.info(f"   ìµœì¢… ì •í™•ë„: {final_performance['accuracy']:.3f}")
    logger.info(f"   ë‚¨ì€ ë¬¸ì œ ì¼€ì´ìŠ¤: {len(final_problems)}ê°œ")


if __name__ == "__main__":
    main() 